Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 22026.472656  [    0/74163]
train loss: 3277.482910  [ 3200/74163]
train loss: 891.645874  [ 6400/74163]
train loss: 441.338318  [ 9600/74163]
train loss: 452.269196  [12800/74163]
train loss: 202.475922  [16000/74163]
train loss: 115.866478  [19200/74163]
train loss: 88.112289  [22400/74163]
train loss: 256.458130  [25600/74163]
train loss: 220.922699  [28800/74163]
train loss: 134.047058  [32000/74163]
train loss: 345.485596  [35200/74163]
train loss: 272.801849  [38400/74163]
train loss: 215.258942  [41600/74163]
train loss: 118.789146  [44800/74163]
train loss: 137.135483  [48000/74163]
train loss: 148.959366  [51200/74163]
train loss: 138.484528  [54400/74163]
train loss: 102.481827  [57600/74163]
train loss: 104.996193  [60800/74163]
train loss: 138.740509  [64000/74163]
train loss: 144.371765  [67200/74163]
train loss: 258.646820  [70400/74163]
train loss: 119.827560  [73600/74163]
Avg validation loss: 158.330790
Epoch 2
-------------------------------
train loss: 150.598969  [    0/74163]
train loss: 186.335663  [ 3200/74163]
train loss: 241.754486  [ 6400/74163]
train loss: 130.455017  [ 9600/74163]
train loss: 110.487015  [12800/74163]
train loss: 105.421997  [16000/74163]
train loss: 126.348839  [19200/74163]
train loss: 92.995544  [22400/74163]
train loss: 94.929405  [25600/74163]
train loss: 66.379410  [28800/74163]
train loss: 68.591644  [32000/74163]
train loss: 147.316086  [35200/74163]
train loss: 157.840485  [38400/74163]
train loss: 195.339325  [41600/74163]
train loss: 162.890579  [44800/74163]
train loss: 87.159889  [48000/74163]
train loss: 102.627380  [51200/74163]
train loss: 111.832542  [54400/74163]
train loss: 85.645348  [57600/74163]
train loss: 173.548355  [60800/74163]
train loss: 155.669525  [64000/74163]
train loss: 97.821739  [67200/74163]
train loss: 167.982040  [70400/74163]
train loss: 87.979462  [73600/74163]
Avg validation loss: 93.516357
Epoch 3
-------------------------------
train loss: 87.681267  [    0/74163]
train loss: 125.438148  [ 3200/74163]
train loss: 131.211395  [ 6400/74163]
train loss: 255.485107  [ 9600/74163]
train loss: 83.369675  [12800/74163]
train loss: 57.082203  [16000/74163]
train loss: 93.167404  [19200/74163]
train loss: 66.400497  [22400/74163]
train loss: 56.775196  [25600/74163]
train loss: 110.233261  [28800/74163]
train loss: 92.328529  [32000/74163]
train loss: 122.105103  [35200/74163]
train loss: 99.425980  [38400/74163]
train loss: 68.195427  [41600/74163]
train loss: 77.494797  [44800/74163]
train loss: 115.816261  [48000/74163]
train loss: 72.949593  [51200/74163]
train loss: 153.327621  [54400/74163]
train loss: 77.225304  [57600/74163]
train loss: 72.921440  [60800/74163]
train loss: 43.002289  [64000/74163]
train loss: 62.573303  [67200/74163]
train loss: 69.331383  [70400/74163]
train loss: 51.983868  [73600/74163]
Avg validation loss: 79.586236
Epoch 4
-------------------------------
train loss: 97.836052  [    0/74163]
train loss: 106.026749  [ 3200/74163]
train loss: 100.786980  [ 6400/74163]
train loss: 71.962570  [ 9600/74163]
train loss: 35.204323  [12800/74163]
train loss: 62.467827  [16000/74163]
train loss: 54.620911  [19200/74163]
train loss: 95.926254  [22400/74163]
train loss: 70.443420  [25600/74163]
train loss: 81.366814  [28800/74163]
train loss: 70.963051  [32000/74163]
train loss: 62.875599  [35200/74163]
train loss: 55.548317  [38400/74163]
train loss: 50.042614  [41600/74163]
train loss: 55.782917  [44800/74163]
train loss: 116.604324  [48000/74163]
train loss: 82.728745  [51200/74163]
train loss: 55.409264  [54400/74163]
train loss: 49.912079  [57600/74163]
train loss: 27.974344  [60800/74163]
train loss: 65.676674  [64000/74163]
train loss: 62.437935  [67200/74163]
train loss: 149.011246  [70400/74163]
train loss: 113.735107  [73600/74163]
Avg validation loss: 61.141214
Epoch 5
-------------------------------
train loss: 110.989578  [    0/74163]
train loss: 58.916862  [ 3200/74163]
train loss: 42.088646  [ 6400/74163]
train loss: 46.455055  [ 9600/74163]
train loss: 53.801575  [12800/74163]
train loss: 73.880798  [16000/74163]
train loss: 73.574173  [19200/74163]
train loss: 66.454315  [22400/74163]
train loss: 40.296852  [25600/74163]
train loss: 72.859932  [28800/74163]
train loss: 59.172619  [32000/74163]
train loss: 38.974327  [35200/74163]
train loss: 77.370384  [38400/74163]
train loss: 48.624084  [41600/74163]
train loss: 55.727734  [44800/74163]
train loss: 60.726391  [48000/74163]
train loss: 36.471291  [51200/74163]
train loss: 30.795166  [54400/74163]
train loss: 43.499313  [57600/74163]
train loss: 36.727104  [60800/74163]
train loss: 78.979385  [64000/74163]
train loss: 37.330524  [67200/74163]
train loss: 104.733109  [70400/74163]
train loss: 39.308235  [73600/74163]
Avg validation loss: 60.519906
Epoch 6
-------------------------------
train loss: 52.827705  [    0/74163]
train loss: 72.419884  [ 3200/74163]
train loss: 42.298939  [ 6400/74163]
train loss: 55.733803  [ 9600/74163]
train loss: 32.634430  [12800/74163]
train loss: 69.563873  [16000/74163]
train loss: 40.367012  [19200/74163]
train loss: 48.683533  [22400/74163]
train loss: 71.078171  [25600/74163]
train loss: 83.848083  [28800/74163]
train loss: 50.410149  [32000/74163]
train loss: 62.778004  [35200/74163]
train loss: 52.365654  [38400/74163]
train loss: 37.657017  [41600/74163]
train loss: 40.015038  [44800/74163]
train loss: 118.614174  [48000/74163]
train loss: 70.617958  [51200/74163]
train loss: 23.564493  [54400/74163]
train loss: 58.513908  [57600/74163]
train loss: 42.329475  [60800/74163]
train loss: 47.137085  [64000/74163]
train loss: 46.513752  [67200/74163]
train loss: 61.481590  [70400/74163]
train loss: 63.006462  [73600/74163]
Avg validation loss: 53.929919
Epoch 7
-------------------------------
train loss: 67.006554  [    0/74163]
train loss: 46.198841  [ 3200/74163]
train loss: 54.183510  [ 6400/74163]
train loss: 78.445023  [ 9600/74163]
train loss: 47.316051  [12800/74163]
train loss: 22.037918  [16000/74163]
train loss: 46.532356  [19200/74163]
train loss: 62.690479  [22400/74163]
train loss: 28.624426  [25600/74163]
train loss: 46.698067  [28800/74163]
train loss: 46.525730  [32000/74163]
train loss: 46.050377  [35200/74163]
train loss: 47.714996  [38400/74163]
train loss: 45.615253  [41600/74163]
train loss: 53.924416  [44800/74163]
train loss: 61.015728  [48000/74163]
train loss: 43.486591  [51200/74163]
train loss: 80.766174  [54400/74163]
train loss: 39.591660  [57600/74163]
train loss: 39.388184  [60800/74163]
train loss: 49.359665  [64000/74163]
train loss: 42.060680  [67200/74163]
train loss: 67.528107  [70400/74163]
train loss: 58.299267  [73600/74163]
Avg validation loss: 52.371758
Epoch 8
-------------------------------
train loss: 52.463661  [    0/74163]
train loss: 30.018747  [ 3200/74163]
train loss: 31.425650  [ 6400/74163]
train loss: 48.725159  [ 9600/74163]
train loss: 52.319195  [12800/74163]
train loss: 40.900852  [16000/74163]
train loss: 63.180275  [19200/74163]
train loss: 25.824299  [22400/74163]
train loss: 40.680519  [25600/74163]
train loss: 68.642387  [28800/74163]
train loss: 54.824677  [32000/74163]
train loss: 50.516212  [35200/74163]
train loss: 67.518715  [38400/74163]
train loss: 34.591183  [41600/74163]
train loss: 52.007477  [44800/74163]
train loss: 47.004753  [48000/74163]
train loss: 61.279003  [51200/74163]
train loss: 54.054798  [54400/74163]
train loss: 53.707527  [57600/74163]
train loss: 45.542351  [60800/74163]
train loss: 58.284069  [64000/74163]
train loss: 57.752686  [67200/74163]
train loss: 40.386082  [70400/74163]
train loss: 48.612286  [73600/74163]
Avg validation loss: 47.014862
Epoch 9
-------------------------------
train loss: 62.700275  [    0/74163]
train loss: 23.876484  [ 3200/74163]
train loss: 51.130219  [ 6400/74163]
train loss: 79.470779  [ 9600/74163]
train loss: 48.877647  [12800/74163]
train loss: 44.133190  [16000/74163]
train loss: 51.012997  [19200/74163]
train loss: 56.038982  [22400/74163]
train loss: 38.020699  [25600/74163]
train loss: 45.893700  [28800/74163]
train loss: 35.673489  [32000/74163]
train loss: 54.463020  [35200/74163]
train loss: 42.762337  [38400/74163]
train loss: 64.293175  [41600/74163]
train loss: 36.463512  [44800/74163]
train loss: 44.537575  [48000/74163]
train loss: 15.893559  [51200/74163]
train loss: 77.926743  [54400/74163]
train loss: 52.600597  [57600/74163]
train loss: 78.138161  [60800/74163]
train loss: 40.820366  [64000/74163]
train loss: 46.356819  [67200/74163]
train loss: 38.750053  [70400/74163]
train loss: 57.307983  [73600/74163]
Avg validation loss: 49.509200
Epoch 10
-------------------------------
train loss: 44.913464  [    0/74163]
train loss: 26.500290  [ 3200/74163]
train loss: 32.695892  [ 6400/74163]
train loss: 50.696003  [ 9600/74163]
train loss: 17.326572  [12800/74163]
train loss: 44.682777  [16000/74163]
train loss: 52.624561  [19200/74163]
train loss: 27.487701  [22400/74163]
train loss: 31.036753  [25600/74163]
train loss: 33.439808  [28800/74163]
train loss: 39.207527  [32000/74163]
train loss: 37.376347  [35200/74163]
train loss: 30.983618  [38400/74163]
train loss: 48.843960  [41600/74163]
train loss: 35.430519  [44800/74163]
train loss: 37.032852  [48000/74163]
train loss: 37.404499  [51200/74163]
train loss: 32.698067  [54400/74163]
train loss: 120.678711  [57600/74163]
train loss: 30.354868  [60800/74163]
train loss: 40.710510  [64000/74163]
train loss: 34.692322  [67200/74163]
train loss: 45.392464  [70400/74163]
train loss: 30.600307  [73600/74163]
Avg validation loss: 49.152033
Done!