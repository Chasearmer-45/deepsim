Shape of X: torch.Size([16, 31])
Shape of y: torch.Size([16, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 7925.067383  [    0/76380]
train loss: 395.014801  [ 1600/76380]
train loss: 135.772049  [ 3200/76380]
train loss: 66.534225  [ 4800/76380]
train loss: 15.903554  [ 6400/76380]
train loss: 17.934605  [ 8000/76380]
train loss: 11.103303  [ 9600/76380]
train loss: 8.809354  [11200/76380]
train loss: 11.676401  [12800/76380]
train loss: 14.153101  [14400/76380]
train loss: 16.859354  [16000/76380]
train loss: 18.518299  [17600/76380]
train loss: 6.951337  [19200/76380]
train loss: 11.332415  [20800/76380]
train loss: 12.774673  [22400/76380]
train loss: 5.298416  [24000/76380]
train loss: 4.848104  [25600/76380]
train loss: 5.718685  [27200/76380]
train loss: 9.879618  [28800/76380]
train loss: 6.703683  [30400/76380]
train loss: 6.093369  [32000/76380]
train loss: 35.026020  [33600/76380]
train loss: 13.844235  [35200/76380]
train loss: 31.117563  [36800/76380]
train loss: 12.317479  [38400/76380]
train loss: 11.252069  [40000/76380]
train loss: 7.105840  [41600/76380]
train loss: 17.784931  [43200/76380]
train loss: 9.842546  [44800/76380]
train loss: 28.540720  [46400/76380]
train loss: 17.593260  [48000/76380]
train loss: 39.271706  [49600/76380]
train loss: 34.832542  [51200/76380]
train loss: 19.294897  [52800/76380]
train loss: 4.197322  [54400/76380]
train loss: 14.364014  [56000/76380]
train loss: 9.186122  [57600/76380]
train loss: 18.343502  [59200/76380]
train loss: 15.043788  [60800/76380]
train loss: 10.062551  [62400/76380]
train loss: 29.150705  [64000/76380]
train loss: 16.798595  [65600/76380]
train loss: 34.090858  [67200/76380]
train loss: 19.233101  [68800/76380]
train loss: 13.423511  [70400/76380]
train loss: 5.348374  [72000/76380]
train loss: 16.416124  [73600/76380]
train loss: 9.053992  [75200/76380]
Avg validation loss: 9.777474
Epoch 2
-------------------------------
train loss: 8.839069  [    0/76380]
train loss: 2.963308  [ 1600/76380]
train loss: 18.173073  [ 3200/76380]
train loss: 18.596289  [ 4800/76380]
train loss: 19.161505  [ 6400/76380]
train loss: 9.568867  [ 8000/76380]
train loss: 6.738673  [ 9600/76380]
train loss: 13.472137  [11200/76380]
train loss: 6.582963  [12800/76380]
train loss: 8.989715  [14400/76380]
train loss: 9.454207  [16000/76380]
train loss: 18.372295  [17600/76380]
train loss: 5.502810  [19200/76380]
train loss: 7.777200  [20800/76380]
train loss: 11.294134  [22400/76380]
train loss: 11.103167  [24000/76380]
train loss: 10.598394  [25600/76380]
train loss: 9.991949  [27200/76380]
train loss: 18.237995  [28800/76380]
train loss: 7.262830  [30400/76380]
train loss: 5.216774  [32000/76380]
train loss: 9.928035  [33600/76380]
train loss: 3.329718  [35200/76380]
train loss: 15.851667  [36800/76380]
train loss: 13.909323  [38400/76380]
train loss: 12.456717  [40000/76380]
train loss: 7.495551  [41600/76380]
train loss: 5.379165  [43200/76380]
train loss: 21.162373  [44800/76380]
train loss: 16.392040  [46400/76380]
train loss: 11.818718  [48000/76380]
train loss: 3.951993  [49600/76380]
train loss: 37.256290  [51200/76380]
train loss: 14.849447  [52800/76380]
train loss: 7.777649  [54400/76380]
train loss: 16.702028  [56000/76380]
train loss: 5.617479  [57600/76380]
train loss: 16.136364  [59200/76380]
train loss: 6.769660  [60800/76380]
train loss: 6.225043  [62400/76380]
train loss: 7.425838  [64000/76380]
train loss: 8.009535  [65600/76380]
train loss: 24.074820  [67200/76380]
train loss: 8.727888  [68800/76380]
train loss: 12.450501  [70400/76380]
train loss: 7.737071  [72000/76380]
train loss: 15.545740  [73600/76380]
train loss: 11.881846  [75200/76380]
Avg validation loss: 12.405320
Epoch 3
-------------------------------
train loss: 10.546727  [    0/76380]
train loss: 12.824257  [ 1600/76380]
train loss: 10.556529  [ 3200/76380]
train loss: 10.024145  [ 4800/76380]
train loss: 10.248590  [ 6400/76380]
train loss: 9.978444  [ 8000/76380]
train loss: 4.903543  [ 9600/76380]
train loss: 7.834158  [11200/76380]
train loss: 8.774577  [12800/76380]
train loss: 13.717321  [14400/76380]
train loss: 10.886403  [16000/76380]
train loss: 5.883127  [17600/76380]
train loss: 12.821164  [19200/76380]
train loss: 15.304547  [20800/76380]
train loss: 241.967987  [22400/76380]
train loss: 7.871157  [24000/76380]
train loss: 7.729442  [25600/76380]
train loss: 15.884939  [27200/76380]
train loss: 15.037882  [28800/76380]
train loss: 22.373226  [30400/76380]
train loss: 8.409316  [32000/76380]
train loss: 5.819518  [33600/76380]
train loss: 5.399634  [35200/76380]
train loss: 15.611399  [36800/76380]
train loss: 4.156388  [38400/76380]
train loss: 9.969507  [40000/76380]
train loss: 6.145831  [41600/76380]
train loss: 14.069296  [43200/76380]
train loss: 5.125589  [44800/76380]
train loss: 4.056054  [46400/76380]
train loss: 79.038689  [48000/76380]
train loss: 8.569193  [49600/76380]
train loss: 6.761414  [51200/76380]
train loss: 27.725857  [52800/76380]
train loss: 6.895360  [54400/76380]
train loss: 7.209300  [56000/76380]
train loss: 28.322727  [57600/76380]
train loss: 9.360493  [59200/76380]
train loss: 8.757494  [60800/76380]
train loss: 8.018992  [62400/76380]
train loss: 4.472764  [64000/76380]
train loss: 7.529418  [65600/76380]
train loss: 6.208954  [67200/76380]
train loss: 9.683061  [68800/76380]
train loss: 9.413687  [70400/76380]
train loss: 7.498771  [72000/76380]
train loss: 17.894108  [73600/76380]
train loss: 13.282614  [75200/76380]
Avg validation loss: 8.723145
Epoch 4
-------------------------------
train loss: 18.420620  [    0/76380]
train loss: 10.489514  [ 1600/76380]
train loss: 9.136863  [ 3200/76380]
train loss: 11.637345  [ 4800/76380]
train loss: 5.973612  [ 6400/76380]
train loss: 8.489181  [ 8000/76380]
train loss: 6.003949  [ 9600/76380]
train loss: 11.487172  [11200/76380]
train loss: 19.374266  [12800/76380]
train loss: 17.308926  [14400/76380]
train loss: 11.828897  [16000/76380]
train loss: 9.129313  [17600/76380]
train loss: 11.396906  [19200/76380]
train loss: 22.724133  [20800/76380]
train loss: 9.743319  [22400/76380]
train loss: 31.665089  [24000/76380]
train loss: 30.296473  [25600/76380]
train loss: 10.122902  [27200/76380]
train loss: 11.850702  [28800/76380]
train loss: 5.867671  [30400/76380]
train loss: 6.727170  [32000/76380]
train loss: 2.297983  [33600/76380]
train loss: 3.583282  [35200/76380]
train loss: 18.245932  [36800/76380]
train loss: 3.351852  [38400/76380]
train loss: 9.926701  [40000/76380]
train loss: 26.781326  [41600/76380]
train loss: 5.805244  [43200/76380]
train loss: 12.516305  [44800/76380]
train loss: 9.195668  [46400/76380]
train loss: 14.253769  [48000/76380]
train loss: 4.628236  [49600/76380]
train loss: 5.498060  [51200/76380]
train loss: 5.268785  [52800/76380]
train loss: 10.336252  [54400/76380]
train loss: 4.551409  [56000/76380]
train loss: 22.280266  [57600/76380]
train loss: 4.493888  [59200/76380]
train loss: 4.874930  [60800/76380]
train loss: 3.499578  [62400/76380]
train loss: 33.199043  [64000/76380]
train loss: 7.836726  [65600/76380]
train loss: 4.409845  [67200/76380]
train loss: 5.765306  [68800/76380]
train loss: 7.402434  [70400/76380]
train loss: 12.940191  [72000/76380]
train loss: 22.997896  [73600/76380]
train loss: 7.210501  [75200/76380]
Avg validation loss: 21.625224
Epoch 5
-------------------------------
train loss: 17.349823  [    0/76380]
train loss: 3.664806  [ 1600/76380]
train loss: 7.765969  [ 3200/76380]
train loss: 4.038854  [ 4800/76380]
train loss: 18.704304  [ 6400/76380]
train loss: 36.626141  [ 8000/76380]
train loss: 4.533180  [ 9600/76380]
train loss: 5.925415  [11200/76380]
train loss: 14.734932  [12800/76380]
train loss: 6.441105  [14400/76380]
train loss: 6.121873  [16000/76380]
train loss: 9.445354  [17600/76380]
train loss: 31.430969  [19200/76380]
train loss: 5.560857  [20800/76380]
train loss: 7.072665  [22400/76380]
train loss: 6.542211  [24000/76380]
train loss: 13.127912  [25600/76380]
train loss: 12.890228  [27200/76380]
train loss: 7.800843  [28800/76380]
train loss: 7.701008  [30400/76380]
train loss: 4.378358  [32000/76380]
train loss: 3.572211  [33600/76380]
train loss: 9.616825  [35200/76380]
train loss: 4.186348  [36800/76380]
train loss: 5.581882  [38400/76380]
train loss: 9.541811  [40000/76380]
train loss: 7.292225  [41600/76380]
train loss: 10.147520  [43200/76380]
train loss: 4.965837  [44800/76380]
train loss: 10.172309  [46400/76380]
train loss: 3.714643  [48000/76380]
train loss: 28.261417  [49600/76380]
train loss: 11.257875  [51200/76380]
train loss: 7.453486  [52800/76380]
train loss: 9.411002  [54400/76380]
train loss: 2.767849  [56000/76380]
train loss: 4.783919  [57600/76380]
train loss: 12.623062  [59200/76380]
train loss: 5.702217  [60800/76380]
train loss: 15.010519  [62400/76380]
train loss: 8.967865  [64000/76380]
train loss: 8.176203  [65600/76380]
train loss: 49.720200  [67200/76380]
train loss: 14.364100  [68800/76380]
train loss: 5.278276  [70400/76380]
train loss: 9.219872  [72000/76380]
train loss: 5.889746  [73600/76380]
train loss: 19.564157  [75200/76380]
Avg validation loss: 9.411311
Epoch 6
-------------------------------
train loss: 5.689909  [    0/76380]
train loss: 7.104750  [ 1600/76380]
train loss: 7.082788  [ 3200/76380]
train loss: 14.694817  [ 4800/76380]
train loss: 5.964312  [ 6400/76380]
train loss: 13.970194  [ 8000/76380]
train loss: 4.342036  [ 9600/76380]
train loss: 11.184172  [11200/76380]
train loss: 6.864718  [12800/76380]
train loss: 7.036267  [14400/76380]
train loss: 3.850061  [16000/76380]
train loss: 3.935716  [17600/76380]
train loss: 4.275014  [19200/76380]
train loss: 6.728891  [20800/76380]
train loss: 9.948557  [22400/76380]
train loss: 9.092734  [24000/76380]
train loss: 6.222051  [25600/76380]
train loss: 7.905541  [27200/76380]
train loss: 21.187073  [28800/76380]
train loss: 4.684279  [30400/76380]
train loss: 1.562466  [32000/76380]
train loss: 35.850399  [33600/76380]
train loss: 10.043738  [35200/76380]
train loss: 9.498091  [36800/76380]
train loss: 8.463702  [38400/76380]
train loss: 7.999971  [40000/76380]
train loss: 3.588527  [41600/76380]
train loss: 14.418692  [43200/76380]
train loss: 7.489671  [44800/76380]
train loss: 7.173522  [46400/76380]
train loss: 6.900439  [48000/76380]
train loss: 7.124816  [49600/76380]
train loss: 14.955028  [51200/76380]
train loss: 12.465875  [52800/76380]
train loss: 2.628284  [54400/76380]
train loss: 20.089325  [56000/76380]
train loss: 5.839733  [57600/76380]
train loss: 5.406698  [59200/76380]
train loss: 3.287825  [60800/76380]
train loss: 7.219905  [62400/76380]
train loss: 35.034218  [64000/76380]
train loss: 7.470766  [65600/76380]
train loss: 3.063919  [67200/76380]
train loss: 7.030609  [68800/76380]
train loss: 7.148253  [70400/76380]
train loss: 6.890831  [72000/76380]
train loss: 4.562593  [73600/76380]
train loss: 8.142627  [75200/76380]
Avg validation loss: 18.012081
Epoch 7
-------------------------------
train loss: 23.654934  [    0/76380]
train loss: 6.686823  [ 1600/76380]
train loss: 4.866191  [ 3200/76380]
train loss: 4.100224  [ 4800/76380]
train loss: 3.333852  [ 6400/76380]
train loss: 13.296651  [ 8000/76380]
train loss: 6.254428  [ 9600/76380]
train loss: 4.143348  [11200/76380]
train loss: 8.085493  [12800/76380]
train loss: 7.094101  [14400/76380]
train loss: 17.758692  [16000/76380]
train loss: 5.342148  [17600/76380]
train loss: 4.250478  [19200/76380]
train loss: 12.590078  [20800/76380]
train loss: 2.493468  [22400/76380]
train loss: 11.883271  [24000/76380]
train loss: 38.225441  [25600/76380]
train loss: 5.194457  [27200/76380]
train loss: 4.368021  [28800/76380]
train loss: 4.215624  [30400/76380]
train loss: 8.945754  [32000/76380]
train loss: 6.663411  [33600/76380]
train loss: 5.266116  [35200/76380]
train loss: 5.540812  [36800/76380]
train loss: 9.462315  [38400/76380]
train loss: 3.847445  [40000/76380]
train loss: 4.369373  [41600/76380]
train loss: 9.748037  [43200/76380]
train loss: 7.107675  [44800/76380]
train loss: 10.436192  [46400/76380]
train loss: 4.195501  [48000/76380]
train loss: 9.619462  [49600/76380]
train loss: 4.136249  [51200/76380]
train loss: 4.260225  [52800/76380]
train loss: 12.236433  [54400/76380]
train loss: 3.150178  [56000/76380]
train loss: 4.336061  [57600/76380]
train loss: 5.597250  [59200/76380]
train loss: 34.992123  [60800/76380]
train loss: 7.176562  [62400/76380]
train loss: 10.055686  [64000/76380]
train loss: 7.799142  [65600/76380]
train loss: 3.286722  [67200/76380]
train loss: 3.677444  [68800/76380]
train loss: 2.997797  [70400/76380]
train loss: 11.734041  [72000/76380]
train loss: 5.855294  [73600/76380]
train loss: 3.650870  [75200/76380]
Avg validation loss: 9.294832
Epoch 8
-------------------------------
train loss: 27.391962  [    0/76380]
train loss: 27.266439  [ 1600/76380]
train loss: 18.249449  [ 3200/76380]
train loss: 14.055681  [ 4800/76380]
train loss: 3.091502  [ 6400/76380]
train loss: 2.916431  [ 8000/76380]
train loss: 10.072693  [ 9600/76380]
train loss: 2.808221  [11200/76380]
train loss: 7.215559  [12800/76380]
train loss: 10.795530  [14400/76380]
train loss: 8.256439  [16000/76380]
train loss: 9.156076  [17600/76380]
train loss: 4.455114  [19200/76380]
train loss: 7.626504  [20800/76380]
train loss: 5.482170  [22400/76380]
train loss: 4.665317  [24000/76380]
train loss: 4.068512  [25600/76380]
train loss: 5.797618  [27200/76380]
train loss: 13.314967  [28800/76380]
train loss: 10.223260  [30400/76380]
train loss: 6.117513  [32000/76380]
train loss: 6.869195  [33600/76380]
train loss: 5.600827  [35200/76380]
train loss: 6.944547  [36800/76380]
train loss: 7.713193  [38400/76380]
train loss: 5.171794  [40000/76380]
train loss: 3.236171  [41600/76380]
train loss: 5.378678  [43200/76380]
train loss: 6.811509  [44800/76380]
train loss: 7.583943  [46400/76380]
train loss: 6.387410  [48000/76380]
train loss: 2.881566  [49600/76380]
train loss: 3.871004  [51200/76380]
train loss: 4.501182  [52800/76380]
train loss: 11.875422  [54400/76380]
train loss: 3.226488  [56000/76380]
train loss: 7.632333  [57600/76380]
train loss: 5.901493  [59200/76380]
train loss: 7.937693  [60800/76380]
train loss: 6.151950  [62400/76380]
train loss: 22.367655  [64000/76380]
train loss: 12.726300  [65600/76380]
train loss: 9.310776  [67200/76380]
train loss: 2.869559  [68800/76380]
train loss: 8.721766  [70400/76380]
train loss: 3.478801  [72000/76380]
train loss: 3.537802  [73600/76380]
train loss: 3.710310  [75200/76380]
Avg validation loss: 6.276796
Epoch 9
-------------------------------
train loss: 6.339709  [    0/76380]
train loss: 5.951121  [ 1600/76380]
train loss: 4.018190  [ 3200/76380]
train loss: 58.235374  [ 4800/76380]
train loss: 5.469157  [ 6400/76380]
train loss: 7.267933  [ 8000/76380]
train loss: 9.302496  [ 9600/76380]
train loss: 4.134156  [11200/76380]
train loss: 1.545261  [12800/76380]
train loss: 8.249139  [14400/76380]
train loss: 7.556442  [16000/76380]
train loss: 3.805428  [17600/76380]
train loss: 3.282660  [19200/76380]
train loss: 4.914750  [20800/76380]
train loss: 3.795188  [22400/76380]
train loss: 2.352028  [24000/76380]
train loss: 4.576121  [25600/76380]
train loss: 9.255906  [27200/76380]
train loss: 27.146618  [28800/76380]
train loss: 4.185458  [30400/76380]
train loss: 10.225094  [32000/76380]
train loss: 21.769125  [33600/76380]
train loss: 4.858235  [35200/76380]
train loss: 5.189968  [36800/76380]
train loss: 2.429364  [38400/76380]
train loss: 2.778187  [40000/76380]
train loss: 2.771106  [41600/76380]
train loss: 6.022210  [43200/76380]
train loss: 7.768016  [44800/76380]
train loss: 16.120508  [46400/76380]
train loss: 9.964573  [48000/76380]
train loss: 11.842375  [49600/76380]
train loss: 4.286315  [51200/76380]
train loss: 4.650144  [52800/76380]
train loss: 4.638098  [54400/76380]
train loss: 16.958914  [56000/76380]
train loss: 8.527308  [57600/76380]
train loss: 6.405746  [59200/76380]
train loss: 15.565317  [60800/76380]
train loss: 3.001945  [62400/76380]
train loss: 9.782748  [64000/76380]
train loss: 4.286647  [65600/76380]
train loss: 4.903220  [67200/76380]
train loss: 5.324927  [68800/76380]
train loss: 3.326887  [70400/76380]
train loss: 2.464691  [72000/76380]
train loss: 5.670841  [73600/76380]
train loss: 4.013278  [75200/76380]
Avg validation loss: 4.256842
Epoch 10
-------------------------------
train loss: 2.659751  [    0/76380]
train loss: 3.129007  [ 1600/76380]
train loss: 4.569018  [ 3200/76380]
train loss: 3.421347  [ 4800/76380]
train loss: 6.417016  [ 6400/76380]
train loss: 3.747367  [ 8000/76380]
train loss: 7.736262  [ 9600/76380]
train loss: 5.374181  [11200/76380]
train loss: 8.363108  [12800/76380]
train loss: 7.256659  [14400/76380]
train loss: 8.037142  [16000/76380]
train loss: 3.162846  [17600/76380]
train loss: 4.510419  [19200/76380]
train loss: 4.067684  [20800/76380]
train loss: 11.901799  [22400/76380]
train loss: 3.816807  [24000/76380]
train loss: 2.421094  [25600/76380]
train loss: 8.369148  [27200/76380]
train loss: 3.994324  [28800/76380]
train loss: 3.053877  [30400/76380]
train loss: 5.283523  [32000/76380]
train loss: 3.956412  [33600/76380]
train loss: 5.090590  [35200/76380]
train loss: 9.749887  [36800/76380]
train loss: 2.782712  [38400/76380]
train loss: 3.413001  [40000/76380]
train loss: 5.299472  [41600/76380]
train loss: 16.915840  [43200/76380]
train loss: 5.165313  [44800/76380]
train loss: 3.237660  [46400/76380]
train loss: 3.277351  [48000/76380]
train loss: 5.839790  [49600/76380]
train loss: 4.599446  [51200/76380]
train loss: 5.331871  [52800/76380]
train loss: 9.099914  [54400/76380]
train loss: 8.379907  [56000/76380]
train loss: 3.697267  [57600/76380]
train loss: 6.657350  [59200/76380]
train loss: 8.440799  [60800/76380]
train loss: 3.388294  [62400/76380]
train loss: 3.887449  [64000/76380]
train loss: 9.233793  [65600/76380]
train loss: 5.928930  [67200/76380]
train loss: 4.948688  [68800/76380]
train loss: 10.755819  [70400/76380]
train loss: 8.340111  [72000/76380]
train loss: 6.150724  [73600/76380]
train loss: 8.349520  [75200/76380]
Avg validation loss: 5.356736
Done!