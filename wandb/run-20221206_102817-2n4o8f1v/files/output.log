Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 6407.926758  [    0/111279]
train loss: 237.767334  [ 3200/111279]
train loss: 158.304977  [ 6400/111279]
train loss: 119.494095  [ 9600/111279]
train loss: 209.404510  [12800/111279]
train loss: 235.568634  [16000/111279]
train loss: 113.343826  [19200/111279]
train loss: 189.486038  [22400/111279]
train loss: 139.042740  [25600/111279]
train loss: 160.320129  [28800/111279]
train loss: 207.920258  [32000/111279]
train loss: 117.448120  [35200/111279]
train loss: 106.923172  [38400/111279]
train loss: 139.478241  [41600/111279]
train loss: 208.431366  [44800/111279]
train loss: 211.930908  [48000/111279]
train loss: 63.125050  [51200/111279]
train loss: 126.042542  [54400/111279]
train loss: 167.805756  [57600/111279]
train loss: 108.481277  [60800/111279]
train loss: 111.264359  [64000/111279]
train loss: 109.864273  [67200/111279]
train loss: 112.039307  [70400/111279]
train loss: 77.290306  [73600/111279]
train loss: 121.103317  [76800/111279]
train loss: 116.688766  [80000/111279]
train loss: 105.464073  [83200/111279]
train loss: 134.773773  [86400/111279]
train loss: 169.110916  [89600/111279]
train loss: 79.100456  [92800/111279]
train loss: 65.822678  [96000/111279]
train loss: 71.454437  [99200/111279]
train loss: 95.858215  [102400/111279]
train loss: 77.393059  [105600/111279]
train loss: 165.268936  [108800/111279]
Avg validation loss: 91.320491
Epoch 2
-------------------------------
train loss: 83.893028  [    0/111279]
train loss: 95.636375  [ 3200/111279]
train loss: 128.788483  [ 6400/111279]
train loss: 65.596581  [ 9600/111279]
train loss: 120.881630  [12800/111279]
train loss: 170.689728  [16000/111279]
train loss: 111.731102  [19200/111279]
train loss: 110.019531  [22400/111279]
train loss: 61.074547  [25600/111279]
train loss: 73.612137  [28800/111279]
train loss: 84.890976  [32000/111279]
train loss: 66.734413  [35200/111279]
train loss: 68.909645  [38400/111279]
train loss: 49.944752  [41600/111279]
train loss: 74.121216  [44800/111279]
train loss: 79.972511  [48000/111279]
train loss: 68.595947  [51200/111279]
train loss: 64.958946  [54400/111279]
train loss: 72.275475  [57600/111279]
train loss: 53.470184  [60800/111279]
train loss: 71.274612  [64000/111279]
train loss: 50.202919  [67200/111279]
train loss: 61.219780  [70400/111279]
train loss: 49.576546  [73600/111279]
train loss: 85.239883  [76800/111279]
train loss: 44.978840  [80000/111279]
train loss: 61.291557  [83200/111279]
train loss: 93.598618  [86400/111279]
train loss: 83.424522  [89600/111279]
train loss: 88.064453  [92800/111279]
train loss: 41.065887  [96000/111279]
train loss: 36.983994  [99200/111279]
train loss: 88.238304  [102400/111279]
train loss: 83.971916  [105600/111279]
train loss: 41.090721  [108800/111279]
Avg validation loss: 57.475215
Epoch 3
-------------------------------
train loss: 52.389050  [    0/111279]
train loss: 91.183228  [ 3200/111279]
train loss: 99.068069  [ 6400/111279]
train loss: 62.877655  [ 9600/111279]
train loss: 103.453369  [12800/111279]
train loss: 65.031242  [16000/111279]
train loss: 57.821419  [19200/111279]
train loss: 66.389183  [22400/111279]
train loss: 44.713097  [25600/111279]
train loss: 55.082447  [28800/111279]
train loss: 43.203747  [32000/111279]
train loss: 53.077187  [35200/111279]
train loss: 44.974434  [38400/111279]
train loss: 68.664406  [41600/111279]
train loss: 35.205154  [44800/111279]
train loss: 47.631649  [48000/111279]
train loss: 32.513515  [51200/111279]
train loss: 51.903992  [54400/111279]
train loss: 63.248791  [57600/111279]
train loss: 39.981598  [60800/111279]
train loss: 44.795837  [64000/111279]
train loss: 44.990074  [67200/111279]
train loss: 51.110909  [70400/111279]
train loss: 33.727474  [73600/111279]
train loss: 56.711037  [76800/111279]
train loss: 59.815338  [80000/111279]
train loss: 40.759521  [83200/111279]
train loss: 30.345139  [86400/111279]
train loss: 41.956318  [89600/111279]
train loss: 46.418816  [92800/111279]
train loss: 41.671276  [96000/111279]
train loss: 99.040222  [99200/111279]
train loss: 48.250450  [102400/111279]
train loss: 39.536232  [105600/111279]
train loss: 54.912086  [108800/111279]
Avg validation loss: 45.608254
Epoch 4
-------------------------------
train loss: 38.091438  [    0/111279]
train loss: 40.686390  [ 3200/111279]
train loss: 64.014122  [ 6400/111279]
train loss: 55.108410  [ 9600/111279]
train loss: 28.219299  [12800/111279]
train loss: 51.295788  [16000/111279]
train loss: 30.684437  [19200/111279]
train loss: 37.553219  [22400/111279]
train loss: 51.817921  [25600/111279]
train loss: 67.143753  [28800/111279]
train loss: 51.528454  [32000/111279]
train loss: 27.961926  [35200/111279]
train loss: 28.476892  [38400/111279]
train loss: 33.903969  [41600/111279]
train loss: 52.144886  [44800/111279]
train loss: 48.462494  [48000/111279]
train loss: 63.484291  [51200/111279]
train loss: 42.400646  [54400/111279]
train loss: 21.570866  [57600/111279]
train loss: 25.952435  [60800/111279]
train loss: 51.309052  [64000/111279]
train loss: 34.086262  [67200/111279]
train loss: 47.763721  [70400/111279]
train loss: 38.919563  [73600/111279]
train loss: 25.623312  [76800/111279]
train loss: 29.156824  [80000/111279]
train loss: 52.240582  [83200/111279]
train loss: 29.450150  [86400/111279]
train loss: 26.311832  [89600/111279]
train loss: 53.346905  [92800/111279]
train loss: 32.599541  [96000/111279]
train loss: 28.617275  [99200/111279]
train loss: 26.335241  [102400/111279]
train loss: 36.072350  [105600/111279]
train loss: 33.599297  [108800/111279]
Avg validation loss: 35.216441
Epoch 5
-------------------------------
train loss: 34.341614  [    0/111279]
train loss: 31.287334  [ 3200/111279]
train loss: 77.310043  [ 6400/111279]
train loss: 38.028580  [ 9600/111279]
train loss: 30.384048  [12800/111279]
train loss: 39.680786  [16000/111279]
train loss: 19.836803  [19200/111279]
train loss: 34.172367  [22400/111279]
train loss: 29.952417  [25600/111279]
train loss: 38.615589  [28800/111279]
train loss: 38.680687  [32000/111279]
train loss: 18.025341  [35200/111279]
train loss: 26.619610  [38400/111279]
train loss: 34.727348  [41600/111279]
train loss: 40.199562  [44800/111279]
train loss: 22.622913  [48000/111279]
train loss: 34.435913  [51200/111279]
train loss: 33.665691  [54400/111279]
train loss: 49.159187  [57600/111279]
train loss: 29.830862  [60800/111279]
train loss: 27.998291  [64000/111279]
train loss: 32.365528  [67200/111279]
train loss: 29.906994  [70400/111279]
train loss: 25.513922  [73600/111279]
train loss: 37.969913  [76800/111279]
train loss: 43.396637  [80000/111279]
train loss: 26.968348  [83200/111279]
train loss: 31.545877  [86400/111279]
train loss: 50.276737  [89600/111279]
train loss: 32.577126  [92800/111279]
train loss: 31.203480  [96000/111279]
train loss: 33.794212  [99200/111279]
train loss: 41.169075  [102400/111279]
train loss: 28.360912  [105600/111279]
train loss: 22.724443  [108800/111279]
Avg validation loss: 32.261782
Epoch 6
-------------------------------
train loss: 36.334755  [    0/111279]
train loss: 16.014654  [ 3200/111279]
train loss: 24.043158  [ 6400/111279]
train loss: 39.518642  [ 9600/111279]
train loss: 20.532187  [12800/111279]
train loss: 32.988583  [16000/111279]
train loss: 27.382862  [19200/111279]
train loss: 25.646332  [22400/111279]
train loss: 28.694403  [25600/111279]
train loss: 21.592464  [28800/111279]
train loss: 35.065727  [32000/111279]
train loss: 27.810703  [35200/111279]
train loss: 37.887802  [38400/111279]
train loss: 36.796978  [41600/111279]
train loss: 23.336445  [44800/111279]
train loss: 23.385410  [48000/111279]
train loss: 35.726646  [51200/111279]
train loss: 26.563488  [54400/111279]
train loss: 47.921970  [57600/111279]
train loss: 28.144434  [60800/111279]
train loss: 35.118469  [64000/111279]
train loss: 28.441828  [67200/111279]
train loss: 32.569565  [70400/111279]
train loss: 33.549385  [73600/111279]
train loss: 28.596279  [76800/111279]
train loss: 23.759764  [80000/111279]
train loss: 23.038149  [83200/111279]
train loss: 31.329229  [86400/111279]
train loss: 29.716822  [89600/111279]
train loss: 48.320271  [92800/111279]
train loss: 22.611656  [96000/111279]
train loss: 30.341684  [99200/111279]
train loss: 22.104691  [102400/111279]
train loss: 45.422813  [105600/111279]
train loss: 28.767788  [108800/111279]
Avg validation loss: 32.003694
Epoch 7
-------------------------------
train loss: 28.770306  [    0/111279]
train loss: 22.545298  [ 3200/111279]
train loss: 30.275421  [ 6400/111279]
train loss: 27.811733  [ 9600/111279]
train loss: 31.874130  [12800/111279]
train loss: 27.185379  [16000/111279]
train loss: 23.698412  [19200/111279]
train loss: 29.229187  [22400/111279]
train loss: 20.939138  [25600/111279]
train loss: 32.444580  [28800/111279]
train loss: 55.070518  [32000/111279]
train loss: 26.802288  [35200/111279]
train loss: 27.844128  [38400/111279]
train loss: 27.243814  [41600/111279]
train loss: 17.107090  [44800/111279]
train loss: 30.866806  [48000/111279]
train loss: 18.178022  [51200/111279]
train loss: 20.590336  [54400/111279]
train loss: 26.348577  [57600/111279]
train loss: 22.929905  [60800/111279]
train loss: 27.246527  [64000/111279]
train loss: 17.100271  [67200/111279]
train loss: 20.270672  [70400/111279]
train loss: 19.991198  [73600/111279]
train loss: 13.746346  [76800/111279]
train loss: 35.288445  [80000/111279]
train loss: 17.110704  [83200/111279]
train loss: 23.770622  [86400/111279]
train loss: 32.254539  [89600/111279]
train loss: 26.559784  [92800/111279]
train loss: 24.238312  [96000/111279]
train loss: 46.314144  [99200/111279]
train loss: 21.659988  [102400/111279]
train loss: 30.564289  [105600/111279]
train loss: 26.283810  [108800/111279]
Avg validation loss: 25.858254
Epoch 8
-------------------------------
train loss: 12.942981  [    0/111279]
train loss: 29.332636  [ 3200/111279]
train loss: 28.518137  [ 6400/111279]
train loss: 39.732761  [ 9600/111279]
train loss: 29.874561  [12800/111279]
train loss: 29.193649  [16000/111279]
train loss: 21.872425  [19200/111279]
train loss: 21.484741  [22400/111279]
train loss: 17.907345  [25600/111279]
train loss: 13.206003  [28800/111279]
train loss: 41.116901  [32000/111279]
train loss: 24.300373  [35200/111279]
train loss: 30.382786  [38400/111279]
train loss: 22.732849  [41600/111279]
train loss: 24.952677  [44800/111279]
train loss: 38.672646  [48000/111279]
train loss: 26.761093  [51200/111279]
train loss: 21.737198  [54400/111279]
train loss: 22.918653  [57600/111279]
train loss: 23.528852  [60800/111279]
train loss: 15.035109  [64000/111279]
train loss: 22.631577  [67200/111279]
train loss: 17.779766  [70400/111279]
train loss: 18.852161  [73600/111279]
train loss: 26.973431  [76800/111279]
train loss: 37.438702  [80000/111279]
train loss: 23.866879  [83200/111279]
train loss: 21.179300  [86400/111279]
train loss: 30.831995  [89600/111279]
train loss: 25.802988  [92800/111279]
train loss: 31.453405  [96000/111279]
train loss: 38.190250  [99200/111279]
train loss: 25.850286  [102400/111279]
train loss: 22.509502  [105600/111279]
train loss: 33.607895  [108800/111279]
Avg validation loss: 25.782805
Epoch 9
-------------------------------
train loss: 24.875111  [    0/111279]
train loss: 24.490551  [ 3200/111279]
train loss: 24.854691  [ 6400/111279]
train loss: 18.514746  [ 9600/111279]
train loss: 18.439228  [12800/111279]
train loss: 23.876774  [16000/111279]
train loss: 35.807621  [19200/111279]
train loss: 21.493340  [22400/111279]
train loss: 24.746712  [25600/111279]
train loss: 16.365086  [28800/111279]
train loss: 21.337116  [32000/111279]
train loss: 23.940420  [35200/111279]
train loss: 21.603237  [38400/111279]
train loss: 20.852741  [41600/111279]
train loss: 19.385460  [44800/111279]
train loss: 31.515686  [48000/111279]
train loss: 26.281052  [51200/111279]
train loss: 37.051598  [54400/111279]
train loss: 25.207174  [57600/111279]
train loss: 24.282362  [60800/111279]
train loss: 28.738245  [64000/111279]
train loss: 24.764507  [67200/111279]
train loss: 17.719549  [70400/111279]
train loss: 23.736729  [73600/111279]
train loss: 33.644676  [76800/111279]
train loss: 35.977928  [80000/111279]
train loss: 31.750559  [83200/111279]
train loss: 24.436371  [86400/111279]
train loss: 30.879803  [89600/111279]
train loss: 23.110189  [92800/111279]
train loss: 18.906353  [96000/111279]
train loss: 15.708303  [99200/111279]
train loss: 40.168797  [102400/111279]
train loss: 25.002819  [105600/111279]
train loss: 26.768946  [108800/111279]
Avg validation loss: 25.739856
Epoch 10
-------------------------------
train loss: 24.757668  [    0/111279]
train loss: 35.693344  [ 3200/111279]
train loss: 25.610922  [ 6400/111279]
train loss: 19.188999  [ 9600/111279]
train loss: 13.643700  [12800/111279]
train loss: 22.167511  [16000/111279]
train loss: 18.121902  [19200/111279]
train loss: 28.007376  [22400/111279]
train loss: 20.912771  [25600/111279]
train loss: 15.889364  [28800/111279]
train loss: 21.988678  [32000/111279]
train loss: 24.495584  [35200/111279]
train loss: 14.431430  [38400/111279]
train loss: 26.001190  [41600/111279]
train loss: 19.079842  [44800/111279]
train loss: 24.897293  [48000/111279]
train loss: 34.371109  [51200/111279]
train loss: 21.636719  [54400/111279]
train loss: 44.130333  [57600/111279]
train loss: 20.909920  [60800/111279]
train loss: 27.359171  [64000/111279]
train loss: 23.704027  [67200/111279]
train loss: 17.561207  [70400/111279]
train loss: 21.119228  [73600/111279]
train loss: 30.251682  [76800/111279]
train loss: 23.521893  [80000/111279]
train loss: 34.774025  [83200/111279]
train loss: 21.797033  [86400/111279]
train loss: 14.611490  [89600/111279]
train loss: 27.996332  [92800/111279]
train loss: 49.644295  [96000/111279]
train loss: 15.086800  [99200/111279]
train loss: 34.709591  [102400/111279]
train loss: 27.333014  [105600/111279]
train loss: 24.152822  [108800/111279]
Avg validation loss: 25.043102
Epoch 11
-------------------------------
train loss: 27.369274  [    0/111279]
train loss: 26.110889  [ 3200/111279]
train loss: 19.502237  [ 6400/111279]
train loss: 16.010290  [ 9600/111279]
train loss: 26.662689  [12800/111279]
train loss: 20.537975  [16000/111279]
train loss: 21.370371  [19200/111279]
train loss: 32.631561  [22400/111279]
train loss: 25.564293  [25600/111279]
train loss: 21.610744  [28800/111279]
train loss: 31.769810  [32000/111279]
train loss: 23.666653  [35200/111279]
train loss: 21.970043  [38400/111279]
train loss: 35.726738  [41600/111279]
train loss: 25.005222  [44800/111279]
train loss: 19.468159  [48000/111279]
train loss: 13.937569  [51200/111279]
train loss: 24.943436  [54400/111279]
train loss: 13.749835  [57600/111279]
train loss: 22.180876  [60800/111279]
train loss: 22.697289  [64000/111279]
train loss: 30.871002  [67200/111279]
train loss: 17.788572  [70400/111279]
train loss: 12.699938  [73600/111279]
train loss: 33.196617  [76800/111279]
train loss: 30.848934  [80000/111279]
train loss: 22.505733  [83200/111279]
train loss: 27.815210  [86400/111279]
train loss: 25.962881  [89600/111279]
train loss: 29.121017  [92800/111279]
train loss: 18.850155  [96000/111279]
train loss: 18.056141  [99200/111279]
train loss: 25.666439  [102400/111279]
train loss: 15.533483  [105600/111279]
train loss: 17.064608  [108800/111279]
Avg validation loss: 24.040443
Epoch 12
-------------------------------
train loss: 33.670357  [    0/111279]
train loss: 26.148235  [ 3200/111279]
train loss: 28.195282  [ 6400/111279]
train loss: 18.932915  [ 9600/111279]
train loss: 21.219570  [12800/111279]
train loss: 28.555719  [16000/111279]
train loss: 23.632063  [19200/111279]
train loss: 19.784405  [22400/111279]
train loss: 19.683401  [25600/111279]
train loss: 46.076912  [28800/111279]
train loss: 24.234371  [32000/111279]
train loss: 15.635936  [35200/111279]
train loss: 21.862446  [38400/111279]
train loss: 20.552647  [41600/111279]
train loss: 31.672668  [44800/111279]
train loss: 32.939144  [48000/111279]
train loss: 24.220377  [51200/111279]
train loss: 19.396826  [54400/111279]
train loss: 17.692585  [57600/111279]
train loss: 22.912113  [60800/111279]
train loss: 20.252640  [64000/111279]
train loss: 15.508873  [67200/111279]
train loss: 12.691467  [70400/111279]
train loss: 32.606060  [73600/111279]
train loss: 18.988358  [76800/111279]
train loss: 23.847309  [80000/111279]
train loss: 26.477112  [83200/111279]
train loss: 33.581684  [86400/111279]
train loss: 30.724874  [89600/111279]
train loss: 15.128105  [92800/111279]
train loss: 28.255161  [96000/111279]
train loss: 16.135656  [99200/111279]
train loss: 20.168842  [102400/111279]
train loss: 14.757240  [105600/111279]
train loss: 14.287859  [108800/111279]
Avg validation loss: 22.917386
Epoch 13
-------------------------------
train loss: 22.434795  [    0/111279]
train loss: 25.251789  [ 3200/111279]
train loss: 23.232412  [ 6400/111279]
train loss: 14.768878  [ 9600/111279]
train loss: 20.975285  [12800/111279]
train loss: 26.247492  [16000/111279]
train loss: 17.633438  [19200/111279]
train loss: 20.485079  [22400/111279]
train loss: 15.707735  [25600/111279]
train loss: 17.463324  [28800/111279]
train loss: 18.958649  [32000/111279]
train loss: 21.878824  [35200/111279]
train loss: 33.930050  [38400/111279]
train loss: 29.849518  [41600/111279]
train loss: 21.641615  [44800/111279]
train loss: 23.342636  [48000/111279]
train loss: 19.096891  [51200/111279]
train loss: 16.100359  [54400/111279]
train loss: 21.279411  [57600/111279]
train loss: 18.915894  [60800/111279]
train loss: 18.301510  [64000/111279]
train loss: 21.211838  [67200/111279]
train loss: 23.153517  [70400/111279]
train loss: 26.554718  [73600/111279]
train loss: 24.693752  [76800/111279]
train loss: 16.490702  [80000/111279]
train loss: 12.372942  [83200/111279]
train loss: 18.781130  [86400/111279]
train loss: 27.657017  [89600/111279]
train loss: 22.528591  [92800/111279]
train loss: 30.312399  [96000/111279]
train loss: 13.544151  [99200/111279]
train loss: 24.401472  [102400/111279]
train loss: 25.358196  [105600/111279]
train loss: 21.514412  [108800/111279]
Avg validation loss: 23.804317
Epoch 14
-------------------------------
train loss: 21.485909  [    0/111279]
train loss: 31.991806  [ 3200/111279]
train loss: 20.971802  [ 6400/111279]
train loss: 20.572268  [ 9600/111279]
train loss: 24.845901  [12800/111279]
train loss: 16.649588  [16000/111279]
train loss: 20.744951  [19200/111279]
train loss: 17.333487  [22400/111279]
train loss: 19.554394  [25600/111279]
train loss: 22.826000  [28800/111279]
train loss: 20.773035  [32000/111279]
train loss: 34.923321  [35200/111279]
train loss: 24.240765  [38400/111279]
train loss: 24.018759  [41600/111279]
train loss: 25.067083  [44800/111279]
train loss: 23.639374  [48000/111279]
train loss: 14.609543  [51200/111279]
train loss: 26.681614  [54400/111279]
train loss: 34.376476  [57600/111279]
train loss: 19.218424  [60800/111279]
train loss: 15.936992  [64000/111279]
train loss: 22.757181  [67200/111279]
train loss: 20.495287  [70400/111279]
train loss: 13.287227  [73600/111279]
train loss: 30.048706  [76800/111279]
train loss: 23.821339  [80000/111279]
train loss: 16.456478  [83200/111279]
train loss: 15.086576  [86400/111279]
train loss: 14.758868  [89600/111279]
train loss: 19.760416  [92800/111279]
train loss: 24.763844  [96000/111279]
train loss: 18.567047  [99200/111279]
train loss: 18.929123  [102400/111279]
train loss: 23.931942  [105600/111279]
train loss: 23.904055  [108800/111279]
Avg validation loss: 21.718209
Epoch 15
-------------------------------
train loss: 32.047718  [    0/111279]
train loss: 18.593151  [ 3200/111279]
train loss: 14.729732  [ 6400/111279]
train loss: 25.486622  [ 9600/111279]
train loss: 16.702524  [12800/111279]
train loss: 27.879953  [16000/111279]
train loss: 17.996157  [19200/111279]
train loss: 24.427490  [22400/111279]
train loss: 20.003315  [25600/111279]
train loss: 33.754723  [28800/111279]
train loss: 20.846828  [32000/111279]
train loss: 35.134670  [35200/111279]
train loss: 12.600122  [38400/111279]
train loss: 24.993605  [41600/111279]
train loss: 20.945255  [44800/111279]
train loss: 14.490965  [48000/111279]
train loss: 16.084751  [51200/111279]
train loss: 27.969982  [54400/111279]
train loss: 11.016297  [57600/111279]
train loss: 13.339079  [60800/111279]
train loss: 15.695865  [64000/111279]
train loss: 22.663826  [67200/111279]
train loss: 13.499187  [70400/111279]
train loss: 19.442528  [73600/111279]
train loss: 14.309000  [76800/111279]
train loss: 11.432704  [80000/111279]
train loss: 23.423595  [83200/111279]
train loss: 14.918332  [86400/111279]
train loss: 21.126678  [89600/111279]
train loss: 30.386456  [92800/111279]
train loss: 17.026989  [96000/111279]
train loss: 30.049946  [99200/111279]
train loss: 30.234476  [102400/111279]
train loss: 21.460419  [105600/111279]
train loss: 11.212061  [108800/111279]
Avg validation loss: 21.601568
Epoch 16
-------------------------------
train loss: 15.026319  [    0/111279]
train loss: 20.679012  [ 3200/111279]
train loss: 18.515602  [ 6400/111279]
train loss: 19.270794  [ 9600/111279]
train loss: 19.774820  [12800/111279]
train loss: 16.935253  [16000/111279]
train loss: 26.384134  [19200/111279]
train loss: 18.743963  [22400/111279]
train loss: 29.878119  [25600/111279]
train loss: 19.574415  [28800/111279]
train loss: 17.561731  [32000/111279]
train loss: 25.210073  [35200/111279]
train loss: 23.648193  [38400/111279]
train loss: 17.956509  [41600/111279]
train loss: 18.351276  [44800/111279]
train loss: 18.332972  [48000/111279]
train loss: 31.753437  [51200/111279]
train loss: 17.898960  [54400/111279]
train loss: 21.297756  [57600/111279]
train loss: 28.212275  [60800/111279]
train loss: 23.923153  [64000/111279]
train loss: 23.250530  [67200/111279]
train loss: 16.018665  [70400/111279]
train loss: 28.164185  [73600/111279]
train loss: 20.373253  [76800/111279]
train loss: 25.961676  [80000/111279]
train loss: 21.708691  [83200/111279]
train loss: 25.848089  [86400/111279]
train loss: 28.461414  [89600/111279]
train loss: 25.166752  [92800/111279]
train loss: 19.776947  [96000/111279]
train loss: 19.076603  [99200/111279]
train loss: 30.790798  [102400/111279]
train loss: 37.914021  [105600/111279]
train loss: 19.755697  [108800/111279]
Avg validation loss: 20.348519
Epoch 17
-------------------------------
train loss: 20.709076  [    0/111279]
train loss: 19.064205  [ 3200/111279]
train loss: 18.139317  [ 6400/111279]
train loss: 15.604361  [ 9600/111279]
train loss: 11.964926  [12800/111279]
train loss: 16.630352  [16000/111279]
train loss: 22.201717  [19200/111279]
train loss: 21.368179  [22400/111279]
train loss: 21.246187  [25600/111279]
train loss: 18.209179  [28800/111279]
train loss: 20.118845  [32000/111279]
train loss: 21.403816  [35200/111279]
train loss: 21.588146  [38400/111279]
train loss: 18.453592  [41600/111279]
train loss: 14.000262  [44800/111279]
train loss: 17.457731  [48000/111279]
train loss: 31.186016  [51200/111279]
train loss: 17.881817  [54400/111279]
train loss: 17.097589  [57600/111279]
train loss: 27.711401  [60800/111279]
train loss: 15.797458  [64000/111279]
train loss: 16.359346  [67200/111279]
train loss: 14.648539  [70400/111279]
train loss: 24.338043  [73600/111279]
train loss: 19.476210  [76800/111279]
train loss: 15.013013  [80000/111279]
train loss: 23.611971  [83200/111279]
train loss: 14.145482  [86400/111279]
train loss: 21.290890  [89600/111279]
train loss: 28.425482  [92800/111279]
train loss: 19.744532  [96000/111279]
train loss: 32.011856  [99200/111279]
train loss: 18.354065  [102400/111279]
train loss: 15.952451  [105600/111279]
train loss: 20.461550  [108800/111279]
Avg validation loss: 21.272015
Epoch 18
-------------------------------
train loss: 21.844086  [    0/111279]
train loss: 22.371441  [ 3200/111279]
train loss: 17.731594  [ 6400/111279]
train loss: 18.671579  [ 9600/111279]
train loss: 18.468372  [12800/111279]
train loss: 14.219599  [16000/111279]
train loss: 21.626169  [19200/111279]
train loss: 36.351898  [22400/111279]
train loss: 19.606499  [25600/111279]
train loss: 16.215876  [28800/111279]
train loss: 20.084894  [32000/111279]
train loss: 21.576395  [35200/111279]
train loss: 19.779858  [38400/111279]
train loss: 13.180031  [41600/111279]
train loss: 13.231975  [44800/111279]
train loss: 21.522057  [48000/111279]
train loss: 16.501965  [51200/111279]
train loss: 29.719162  [54400/111279]
train loss: 17.271437  [57600/111279]
train loss: 22.694273  [60800/111279]
train loss: 12.624968  [64000/111279]
train loss: 19.045097  [67200/111279]
train loss: 22.552776  [70400/111279]
train loss: 24.948469  [73600/111279]
train loss: 14.860478  [76800/111279]
train loss: 20.078861  [80000/111279]
train loss: 16.379553  [83200/111279]
train loss: 16.364985  [86400/111279]
train loss: 18.196877  [89600/111279]
train loss: 11.734724  [92800/111279]
train loss: 16.438423  [96000/111279]
train loss: 12.893575  [99200/111279]
train loss: 24.628729  [102400/111279]
train loss: 18.734900  [105600/111279]
train loss: 23.966494  [108800/111279]
Avg validation loss: 19.513582
Epoch 19
-------------------------------
train loss: 16.829586  [    0/111279]
train loss: 29.204498  [ 3200/111279]
train loss: 36.662376  [ 6400/111279]
train loss: 12.461093  [ 9600/111279]
train loss: 23.640245  [12800/111279]
train loss: 28.084040  [16000/111279]
train loss: 21.047356  [19200/111279]
train loss: 17.623915  [22400/111279]
train loss: 23.014317  [25600/111279]
train loss: 17.510426  [28800/111279]
train loss: 20.623835  [32000/111279]
train loss: 19.988163  [35200/111279]
train loss: 16.969978  [38400/111279]
train loss: 21.478313  [41600/111279]
train loss: 20.133684  [44800/111279]
train loss: 16.962744  [48000/111279]
train loss: 19.751965  [51200/111279]
train loss: 20.098402  [54400/111279]
train loss: 14.470540  [57600/111279]
train loss: 17.448008  [60800/111279]
train loss: 16.190699  [64000/111279]
train loss: 17.428303  [67200/111279]
train loss: 14.206636  [70400/111279]
train loss: 20.998434  [73600/111279]
train loss: 24.451046  [76800/111279]
train loss: 21.106005  [80000/111279]
train loss: 16.770350  [83200/111279]
train loss: 25.866802  [86400/111279]
train loss: 23.572968  [89600/111279]
train loss: 28.344837  [92800/111279]
train loss: 18.668812  [96000/111279]
train loss: 19.528337  [99200/111279]
train loss: 21.492304  [102400/111279]
train loss: 23.275936  [105600/111279]
train loss: 16.325352  [108800/111279]
Avg validation loss: 18.974157
Epoch 20
-------------------------------
train loss: 27.362146  [    0/111279]
train loss: 21.824062  [ 3200/111279]
train loss: 15.525503  [ 6400/111279]
train loss: 18.116995  [ 9600/111279]
train loss: 26.775431  [12800/111279]
train loss: 26.530634  [16000/111279]
train loss: 16.072794  [19200/111279]
train loss: 22.767429  [22400/111279]
train loss: 26.413013  [25600/111279]
train loss: 29.771011  [28800/111279]
train loss: 15.281323  [32000/111279]
train loss: 22.770435  [35200/111279]
train loss: 15.824076  [38400/111279]
train loss: 23.599602  [41600/111279]
train loss: 23.582195  [44800/111279]
train loss: 16.272282  [48000/111279]
train loss: 38.521614  [51200/111279]
train loss: 17.968590  [54400/111279]
train loss: 22.621685  [57600/111279]
train loss: 14.404581  [60800/111279]
train loss: 19.443285  [64000/111279]
train loss: 19.239246  [67200/111279]
train loss: 20.682459  [70400/111279]
train loss: 13.981798  [73600/111279]
train loss: 21.888271  [76800/111279]
train loss: 16.662598  [80000/111279]
train loss: 14.740828  [83200/111279]
train loss: 11.241949  [86400/111279]
train loss: 17.527752  [89600/111279]
train loss: 18.707197  [92800/111279]
train loss: 10.566324  [96000/111279]
train loss: 16.447865  [99200/111279]
train loss: 20.320194  [102400/111279]
train loss: 14.964827  [105600/111279]
train loss: 16.964926  [108800/111279]
Avg validation loss: 19.264649
Done!