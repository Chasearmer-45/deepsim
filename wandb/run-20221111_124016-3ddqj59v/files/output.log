Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 5691.221680  [    0/73120]
train loss: 1952.757202  [ 3200/73120]
train loss: 201.854202  [ 6400/73120]
train loss: 55.864052  [ 9600/73120]
train loss: 53.203411  [12800/73120]
train loss: 28.131495  [16000/73120]
train loss: 21.599661  [19200/73120]
train loss: 28.409225  [22400/73120]
train loss: 21.920887  [25600/73120]
train loss: 19.706818  [28800/73120]
train loss: 23.426437  [32000/73120]
train loss: 16.351089  [35200/73120]
train loss: 36.364769  [38400/73120]
train loss: 20.657837  [41600/73120]
train loss: 22.379696  [44800/73120]
train loss: 31.748291  [48000/73120]
train loss: 26.229025  [51200/73120]
train loss: 16.018583  [54400/73120]
train loss: 17.279362  [57600/73120]
train loss: 26.622890  [60800/73120]
train loss: 26.001978  [64000/73120]
train loss: 24.906694  [67200/73120]
train loss: 19.126877  [70400/73120]
Avg validation loss: 19.834381
Epoch 2
-------------------------------
train loss: 17.330694  [    0/73120]
train loss: 29.106314  [ 3200/73120]
train loss: 23.884380  [ 6400/73120]
train loss: 17.502275  [ 9600/73120]
train loss: 17.171906  [12800/73120]
train loss: 27.489323  [16000/73120]
train loss: 32.691025  [19200/73120]
train loss: 14.748270  [22400/73120]
train loss: 27.397739  [25600/73120]
train loss: 12.798539  [28800/73120]
train loss: 16.799875  [32000/73120]
train loss: 19.622845  [35200/73120]
train loss: 12.346235  [38400/73120]
train loss: 20.098082  [41600/73120]
train loss: 17.125668  [44800/73120]
train loss: 15.779348  [48000/73120]
train loss: 15.472575  [51200/73120]
train loss: 14.100912  [54400/73120]
train loss: 15.729147  [57600/73120]
train loss: 12.978378  [60800/73120]
train loss: 19.445730  [64000/73120]
train loss: 25.129326  [67200/73120]
train loss: 16.200336  [70400/73120]
Avg validation loss: 20.533724
Epoch 3
-------------------------------
train loss: 16.197010  [    0/73120]
train loss: 16.876598  [ 3200/73120]
train loss: 16.408497  [ 6400/73120]
train loss: 15.476456  [ 9600/73120]
train loss: 46.448730  [12800/73120]
train loss: 19.224367  [16000/73120]
train loss: 10.891675  [19200/73120]
train loss: 12.651479  [22400/73120]
train loss: 15.049711  [25600/73120]
train loss: 16.816698  [28800/73120]
train loss: 18.520657  [32000/73120]
train loss: 12.553115  [35200/73120]
train loss: 8.178091  [38400/73120]
train loss: 21.603188  [41600/73120]
train loss: 15.792384  [44800/73120]
train loss: 17.733742  [48000/73120]
train loss: 10.956531  [51200/73120]
train loss: 14.091690  [54400/73120]
train loss: 13.032204  [57600/73120]
train loss: 15.701273  [60800/73120]
train loss: 10.849612  [64000/73120]
train loss: 23.341242  [67200/73120]
train loss: 17.914989  [70400/73120]
Avg validation loss: 14.922097
Epoch 4
-------------------------------
train loss: 25.458536  [    0/73120]
train loss: 30.964207  [ 3200/73120]
train loss: 9.372932  [ 6400/73120]
train loss: 17.002094  [ 9600/73120]
train loss: 28.987289  [12800/73120]
train loss: 11.998924  [16000/73120]
train loss: 14.790003  [19200/73120]
train loss: 8.873899  [22400/73120]
train loss: 5.193431  [25600/73120]
train loss: 13.555479  [28800/73120]
train loss: 11.271235  [32000/73120]
train loss: 10.449286  [35200/73120]
train loss: 11.840061  [38400/73120]
train loss: 37.412758  [41600/73120]
train loss: 9.020238  [44800/73120]
train loss: 8.437117  [48000/73120]
train loss: 16.855083  [51200/73120]
train loss: 10.195628  [54400/73120]
train loss: 13.195885  [57600/73120]
train loss: 13.058138  [60800/73120]
train loss: 32.702080  [64000/73120]
train loss: 7.105811  [67200/73120]
train loss: 16.659578  [70400/73120]
Avg validation loss: 13.023440
Epoch 5
-------------------------------
train loss: 6.257940  [    0/73120]
train loss: 23.483158  [ 3200/73120]
train loss: 24.510986  [ 6400/73120]
train loss: 7.191419  [ 9600/73120]
train loss: 12.374605  [12800/73120]
train loss: 21.420982  [16000/73120]
train loss: 7.799942  [19200/73120]
train loss: 19.311100  [22400/73120]
train loss: 11.666600  [25600/73120]
train loss: 17.074375  [28800/73120]
train loss: 24.976440  [32000/73120]
train loss: 11.031456  [35200/73120]
train loss: 10.830976  [38400/73120]
train loss: 23.670238  [41600/73120]
train loss: 9.399357  [44800/73120]
train loss: 22.271820  [48000/73120]
train loss: 10.543927  [51200/73120]
train loss: 9.822497  [54400/73120]
train loss: 10.859312  [57600/73120]
train loss: 23.710039  [60800/73120]
train loss: 12.158915  [64000/73120]
train loss: 9.642915  [67200/73120]
train loss: 10.031729  [70400/73120]
Avg validation loss: 11.909538
Epoch 6
-------------------------------
train loss: 11.217757  [    0/73120]
train loss: 11.875033  [ 3200/73120]
train loss: 9.185083  [ 6400/73120]
train loss: 11.729116  [ 9600/73120]
train loss: 18.195616  [12800/73120]
train loss: 8.598735  [16000/73120]
train loss: 12.692479  [19200/73120]
train loss: 11.553555  [22400/73120]
train loss: 21.258724  [25600/73120]
train loss: 18.005184  [28800/73120]
train loss: 10.772416  [32000/73120]
train loss: 12.182711  [35200/73120]
train loss: 11.598568  [38400/73120]
train loss: 8.986178  [41600/73120]
train loss: 13.146129  [44800/73120]
train loss: 10.787148  [48000/73120]
train loss: 10.179110  [51200/73120]
train loss: 13.829274  [54400/73120]
train loss: 9.017733  [57600/73120]
train loss: 17.026985  [60800/73120]
train loss: 9.732811  [64000/73120]
train loss: 9.697067  [67200/73120]
train loss: 11.519941  [70400/73120]
Avg validation loss: 13.475073
Epoch 7
-------------------------------
train loss: 12.335518  [    0/73120]
train loss: 11.821946  [ 3200/73120]
train loss: 8.880071  [ 6400/73120]
train loss: 10.801341  [ 9600/73120]
train loss: 9.494298  [12800/73120]
train loss: 7.314355  [16000/73120]
train loss: 16.029665  [19200/73120]
train loss: 22.180431  [22400/73120]
train loss: 16.952145  [25600/73120]
train loss: 10.224407  [28800/73120]
train loss: 13.486753  [32000/73120]
train loss: 6.197383  [35200/73120]
train loss: 10.030396  [38400/73120]
train loss: 7.224888  [41600/73120]
train loss: 8.695513  [44800/73120]
train loss: 6.502119  [48000/73120]
train loss: 13.528635  [51200/73120]
train loss: 13.970080  [54400/73120]
train loss: 18.789099  [57600/73120]
train loss: 7.567381  [60800/73120]
train loss: 9.877890  [64000/73120]
train loss: 9.608228  [67200/73120]
train loss: 20.010052  [70400/73120]
Avg validation loss: 11.052246
Epoch 8
-------------------------------
train loss: 9.380572  [    0/73120]
train loss: 12.811772  [ 3200/73120]
train loss: 9.416595  [ 6400/73120]
train loss: 15.413852  [ 9600/73120]
train loss: 16.387625  [12800/73120]
train loss: 6.220788  [16000/73120]
train loss: 14.581352  [19200/73120]
train loss: 9.197130  [22400/73120]
train loss: 7.074519  [25600/73120]
train loss: 13.130939  [28800/73120]
train loss: 8.325296  [32000/73120]
train loss: 11.740953  [35200/73120]
train loss: 11.489358  [38400/73120]
train loss: 14.983810  [41600/73120]
train loss: 11.438219  [44800/73120]
train loss: 10.339804  [48000/73120]
train loss: 9.891271  [51200/73120]
train loss: 13.722857  [54400/73120]
train loss: 7.730143  [57600/73120]
train loss: 10.451906  [60800/73120]
train loss: 7.419055  [64000/73120]
train loss: 6.786570  [67200/73120]
train loss: 8.545218  [70400/73120]
Avg validation loss: 10.195171
Epoch 9
-------------------------------
train loss: 7.634528  [    0/73120]
train loss: 9.556636  [ 3200/73120]
train loss: 9.589035  [ 6400/73120]
train loss: 14.887708  [ 9600/73120]
train loss: 6.107896  [12800/73120]
train loss: 7.976558  [16000/73120]
train loss: 8.128016  [19200/73120]
train loss: 12.319719  [22400/73120]
train loss: 11.384081  [25600/73120]
train loss: 8.241714  [28800/73120]
train loss: 12.960042  [32000/73120]
train loss: 5.979696  [35200/73120]
train loss: 7.830180  [38400/73120]
train loss: 9.645902  [41600/73120]
train loss: 6.858891  [44800/73120]
train loss: 7.667992  [48000/73120]
train loss: 12.655199  [51200/73120]
train loss: 5.355278  [54400/73120]
train loss: 7.229601  [57600/73120]
train loss: 9.646937  [60800/73120]
train loss: 10.575040  [64000/73120]
train loss: 13.320335  [67200/73120]
train loss: 12.030236  [70400/73120]
Avg validation loss: 7.820971
Epoch 10
-------------------------------
train loss: 6.275194  [    0/73120]
train loss: 11.137057  [ 3200/73120]
train loss: 14.334146  [ 6400/73120]
train loss: 7.205267  [ 9600/73120]
train loss: 8.703244  [12800/73120]
train loss: 8.609491  [16000/73120]
train loss: 9.821246  [19200/73120]
train loss: 9.267517  [22400/73120]
train loss: 8.732054  [25600/73120]
train loss: 8.120504  [28800/73120]
train loss: 12.870526  [32000/73120]
train loss: 4.416798  [35200/73120]
train loss: 4.838249  [38400/73120]
train loss: 11.426727  [41600/73120]
train loss: 6.090056  [44800/73120]
train loss: 10.545684  [48000/73120]
train loss: 18.822983  [51200/73120]
train loss: 14.358255  [54400/73120]
train loss: 7.105051  [57600/73120]
train loss: 8.626234  [60800/73120]
train loss: 11.098247  [64000/73120]
train loss: 6.845641  [67200/73120]
train loss: 10.751992  [70400/73120]
Avg validation loss: 9.010443
Done!