Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 8253.459961  [    0/76380]
train loss: 365.220398  [ 3200/76380]
train loss: 128.502472  [ 6400/76380]
train loss: 80.245171  [ 9600/76380]
train loss: 29.744629  [12800/76380]
train loss: 13.152365  [16000/76380]
train loss: 13.843435  [19200/76380]
train loss: 7.779648  [22400/76380]
train loss: 9.401293  [25600/76380]
train loss: 13.310890  [28800/76380]
train loss: 12.343966  [32000/76380]
train loss: 10.117628  [35200/76380]
train loss: 14.701222  [38400/76380]
train loss: 17.379457  [41600/76380]
train loss: 9.209085  [44800/76380]
train loss: 5.700284  [48000/76380]
train loss: 10.339346  [51200/76380]
train loss: 6.995857  [54400/76380]
train loss: 10.276340  [57600/76380]
train loss: 9.990083  [60800/76380]
train loss: 17.676891  [64000/76380]
train loss: 79.628929  [67200/76380]
train loss: 4.555302  [70400/76380]
train loss: 10.898781  [73600/76380]
Avg validation loss: 7.700934
Epoch 2
-------------------------------
train loss: 7.125057  [    0/76380]
train loss: 5.215038  [ 3200/76380]
train loss: 8.852173  [ 6400/76380]
train loss: 4.987995  [ 9600/76380]
train loss: 7.914342  [12800/76380]
train loss: 13.996561  [16000/76380]
train loss: 11.756750  [19200/76380]
train loss: 6.005984  [22400/76380]
train loss: 4.744754  [25600/76380]
train loss: 8.657493  [28800/76380]
train loss: 9.762964  [32000/76380]
train loss: 12.122690  [35200/76380]
train loss: 6.287285  [38400/76380]
train loss: 4.950270  [41600/76380]
train loss: 4.684710  [44800/76380]
train loss: 6.068783  [48000/76380]
train loss: 4.681538  [51200/76380]
train loss: 4.359172  [54400/76380]
train loss: 8.597692  [57600/76380]
train loss: 5.974708  [60800/76380]
train loss: 9.480050  [64000/76380]
train loss: 6.641854  [67200/76380]
train loss: 5.418831  [70400/76380]
train loss: 11.775951  [73600/76380]
Avg validation loss: 12.083834
Epoch 3
-------------------------------
train loss: 10.136647  [    0/76380]
train loss: 8.601557  [ 3200/76380]
train loss: 8.743582  [ 6400/76380]
train loss: 12.785522  [ 9600/76380]
train loss: 12.448129  [12800/76380]
train loss: 7.523386  [16000/76380]
train loss: 9.066942  [19200/76380]
train loss: 7.977122  [22400/76380]
train loss: 7.116062  [25600/76380]
train loss: 4.941255  [28800/76380]
train loss: 12.859273  [32000/76380]
train loss: 6.786057  [35200/76380]
train loss: 5.814069  [38400/76380]
train loss: 14.854533  [41600/76380]
train loss: 11.205882  [44800/76380]
train loss: 8.825510  [48000/76380]
train loss: 8.464813  [51200/76380]
train loss: 6.771916  [54400/76380]
train loss: 8.583492  [57600/76380]
train loss: 10.324455  [60800/76380]
train loss: 5.475206  [64000/76380]
train loss: 11.865140  [67200/76380]
train loss: 9.935324  [70400/76380]
train loss: 5.142135  [73600/76380]
Avg validation loss: 7.678149
Epoch 4
-------------------------------
train loss: 5.082822  [    0/76380]
train loss: 16.319479  [ 3200/76380]
train loss: 11.236589  [ 6400/76380]
train loss: 6.565299  [ 9600/76380]
train loss: 5.333165  [12800/76380]
train loss: 18.010674  [16000/76380]
train loss: 5.702973  [19200/76380]
train loss: 6.813734  [22400/76380]
train loss: 16.453018  [25600/76380]
train loss: 11.647809  [28800/76380]
train loss: 8.915539  [32000/76380]
train loss: 5.692385  [35200/76380]
train loss: 13.233261  [38400/76380]
train loss: 5.474742  [41600/76380]
train loss: 7.662473  [44800/76380]
train loss: 6.901601  [48000/76380]
train loss: 12.340403  [51200/76380]
train loss: 8.324228  [54400/76380]
train loss: 6.019895  [57600/76380]
train loss: 15.569420  [60800/76380]
train loss: 12.276149  [64000/76380]
train loss: 6.731397  [67200/76380]
train loss: 4.273097  [70400/76380]
train loss: 4.151459  [73600/76380]
Avg validation loss: 5.703574
Epoch 5
-------------------------------
train loss: 2.940050  [    0/76380]
train loss: 17.053265  [ 3200/76380]
train loss: 4.464513  [ 6400/76380]
train loss: 6.221877  [ 9600/76380]
train loss: 18.641609  [12800/76380]
train loss: 12.862961  [16000/76380]
train loss: 3.774682  [19200/76380]
train loss: 7.109477  [22400/76380]
train loss: 17.973959  [25600/76380]
train loss: 5.743083  [28800/76380]
train loss: 5.091762  [32000/76380]
train loss: 10.114546  [35200/76380]
train loss: 13.184580  [38400/76380]
train loss: 6.995988  [41600/76380]
train loss: 5.425632  [44800/76380]
train loss: 8.516106  [48000/76380]
train loss: 7.094565  [51200/76380]
train loss: 11.277920  [54400/76380]
train loss: 4.401492  [57600/76380]
train loss: 7.061066  [60800/76380]
train loss: 8.378917  [64000/76380]
train loss: 3.403957  [67200/76380]
train loss: 6.093514  [70400/76380]
train loss: 5.401944  [73600/76380]
Avg validation loss: 8.210956
Epoch 6
-------------------------------
train loss: 10.489717  [    0/76380]
train loss: 7.077592  [ 3200/76380]
train loss: 9.687614  [ 6400/76380]
train loss: 9.846079  [ 9600/76380]
train loss: 3.438276  [12800/76380]
train loss: 6.520730  [16000/76380]
train loss: 10.406809  [19200/76380]
train loss: 6.909606  [22400/76380]
train loss: 6.297011  [25600/76380]
train loss: 3.297804  [28800/76380]
train loss: 9.072761  [32000/76380]
train loss: 5.106137  [35200/76380]
train loss: 9.538198  [38400/76380]
train loss: 8.631504  [41600/76380]
train loss: 4.603386  [44800/76380]
train loss: 3.437313  [48000/76380]
train loss: 82.550819  [51200/76380]
train loss: 8.762826  [54400/76380]
train loss: 5.631172  [57600/76380]
train loss: 6.469069  [60800/76380]
train loss: 3.111670  [64000/76380]
train loss: 6.603577  [67200/76380]
train loss: 6.160848  [70400/76380]
train loss: 5.275302  [73600/76380]
Avg validation loss: 6.257445
Epoch 7
-------------------------------
train loss: 4.505983  [    0/76380]
train loss: 6.906296  [ 3200/76380]
train loss: 8.941482  [ 6400/76380]
train loss: 5.371167  [ 9600/76380]
train loss: 4.654630  [12800/76380]
train loss: 5.935174  [16000/76380]
train loss: 9.705596  [19200/76380]
train loss: 3.582820  [22400/76380]
train loss: 5.796958  [25600/76380]
train loss: 5.101514  [28800/76380]
train loss: 5.041346  [32000/76380]
train loss: 6.961806  [35200/76380]
train loss: 2.424401  [38400/76380]
train loss: 3.611780  [41600/76380]
train loss: 7.003470  [44800/76380]
train loss: 3.426806  [48000/76380]
train loss: 10.742831  [51200/76380]
train loss: 7.104186  [54400/76380]
train loss: 5.856410  [57600/76380]
train loss: 5.735660  [60800/76380]
train loss: 9.294172  [64000/76380]
train loss: 4.373750  [67200/76380]
train loss: 4.293878  [70400/76380]
train loss: 17.206057  [73600/76380]
Avg validation loss: 6.008726
Epoch 8
-------------------------------
train loss: 4.584257  [    0/76380]
train loss: 5.260962  [ 3200/76380]
train loss: 5.224327  [ 6400/76380]
train loss: 5.310878  [ 9600/76380]
train loss: 8.987535  [12800/76380]
train loss: 4.291013  [16000/76380]
train loss: 4.410380  [19200/76380]
train loss: 5.761737  [22400/76380]
train loss: 7.368613  [25600/76380]
train loss: 10.557414  [28800/76380]
train loss: 4.919782  [32000/76380]
train loss: 7.208249  [35200/76380]
train loss: 6.035918  [38400/76380]
train loss: 4.474370  [41600/76380]
train loss: 3.608386  [44800/76380]
train loss: 5.411942  [48000/76380]
train loss: 2.924230  [51200/76380]
train loss: 7.475179  [54400/76380]
train loss: 28.516628  [57600/76380]
train loss: 18.895933  [60800/76380]
train loss: 4.788177  [64000/76380]
train loss: 3.252970  [67200/76380]
train loss: 5.279132  [70400/76380]
train loss: 3.704483  [73600/76380]
Avg validation loss: 4.750823
Epoch 9
-------------------------------
train loss: 4.302419  [    0/76380]
train loss: 6.103280  [ 3200/76380]
train loss: 5.449102  [ 6400/76380]
train loss: 6.330239  [ 9600/76380]
train loss: 5.461655  [12800/76380]
train loss: 5.428756  [16000/76380]
train loss: 2.961386  [19200/76380]
train loss: 7.779292  [22400/76380]
train loss: 8.564127  [25600/76380]
train loss: 6.151114  [28800/76380]
train loss: 4.804888  [32000/76380]
train loss: 6.089879  [35200/76380]
train loss: 3.899214  [38400/76380]
train loss: 2.866066  [41600/76380]
train loss: 4.137528  [44800/76380]
train loss: 12.662412  [48000/76380]
train loss: 7.151965  [51200/76380]
train loss: 6.507360  [54400/76380]
train loss: 3.665247  [57600/76380]
train loss: 5.205339  [60800/76380]
train loss: 9.097733  [64000/76380]
train loss: 5.061566  [67200/76380]
train loss: 3.144866  [70400/76380]
train loss: 4.927421  [73600/76380]
Avg validation loss: 3.931595
Epoch 10
-------------------------------
train loss: 2.869169  [    0/76380]
train loss: 7.111491  [ 3200/76380]
train loss: 6.415664  [ 6400/76380]
train loss: 2.638738  [ 9600/76380]
train loss: 6.769952  [12800/76380]
train loss: 4.349087  [16000/76380]
train loss: 6.358499  [19200/76380]
train loss: 6.083369  [22400/76380]
train loss: 3.639406  [25600/76380]
train loss: 4.161845  [28800/76380]
train loss: 7.993563  [32000/76380]
train loss: 5.347030  [35200/76380]
train loss: 2.779469  [38400/76380]
train loss: 2.956270  [41600/76380]
train loss: 6.592561  [44800/76380]
train loss: 2.260309  [48000/76380]
train loss: 4.785353  [51200/76380]
train loss: 3.297484  [54400/76380]
train loss: 7.370258  [57600/76380]
train loss: 6.412465  [60800/76380]
train loss: 4.620290  [64000/76380]
train loss: 3.467507  [67200/76380]
train loss: 3.461274  [70400/76380]
train loss: 3.183927  [73600/76380]
Avg validation loss: 4.833480
Done!