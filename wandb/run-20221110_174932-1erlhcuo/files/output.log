Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 11445.772461  [    0/76380]
train loss: 1286.358154  [ 3200/76380]
train loss: 292.097076  [ 6400/76380]
train loss: 66.625320  [ 9600/76380]
train loss: 25.007795  [12800/76380]
train loss: 25.182842  [16000/76380]
train loss: 11.180766  [19200/76380]
train loss: 12.675186  [22400/76380]
train loss: 9.225344  [25600/76380]
train loss: 8.544174  [28800/76380]
train loss: 10.955319  [32000/76380]
train loss: 15.379927  [35200/76380]
train loss: 15.520099  [38400/76380]
train loss: 9.578302  [41600/76380]
train loss: 14.498682  [44800/76380]
train loss: 6.239268  [48000/76380]
train loss: 10.533701  [51200/76380]
train loss: 16.076309  [54400/76380]
train loss: 10.224054  [57600/76380]
train loss: 9.065371  [60800/76380]
train loss: 8.023466  [64000/76380]
train loss: 5.660296  [67200/76380]
train loss: 12.271393  [70400/76380]
train loss: 11.101652  [73600/76380]
Avg validation loss: 8.071421
Epoch 2
-------------------------------
train loss: 6.373350  [    0/76380]
train loss: 9.063654  [ 3200/76380]
train loss: 13.854349  [ 6400/76380]
train loss: 18.260540  [ 9600/76380]
train loss: 13.032361  [12800/76380]
train loss: 6.870545  [16000/76380]
train loss: 5.523179  [19200/76380]
train loss: 7.146948  [22400/76380]
train loss: 7.078270  [25600/76380]
train loss: 10.984372  [28800/76380]
train loss: 7.713023  [32000/76380]
train loss: 7.138439  [35200/76380]
train loss: 7.150156  [38400/76380]
train loss: 6.253687  [41600/76380]
train loss: 6.996624  [44800/76380]
train loss: 7.478517  [48000/76380]
train loss: 5.286995  [51200/76380]
train loss: 6.091676  [54400/76380]
train loss: 11.653406  [57600/76380]
train loss: 10.733854  [60800/76380]
train loss: 8.284726  [64000/76380]
train loss: 6.436910  [67200/76380]
train loss: 14.044893  [70400/76380]
train loss: 12.291188  [73600/76380]
Avg validation loss: 9.063240
Epoch 3
-------------------------------
train loss: 17.613323  [    0/76380]
train loss: 8.442777  [ 3200/76380]
train loss: 8.023001  [ 6400/76380]
train loss: 15.435771  [ 9600/76380]
train loss: 4.765493  [12800/76380]
train loss: 6.711191  [16000/76380]
train loss: 5.959383  [19200/76380]
train loss: 14.396133  [22400/76380]
train loss: 4.826674  [25600/76380]
train loss: 3.758021  [28800/76380]
train loss: 8.617376  [32000/76380]
train loss: 4.562585  [35200/76380]
train loss: 29.914486  [38400/76380]
train loss: 7.766654  [41600/76380]
train loss: 15.137660  [44800/76380]
train loss: 4.151700  [48000/76380]
train loss: 3.990049  [51200/76380]
train loss: 16.858604  [54400/76380]
train loss: 5.408708  [57600/76380]
train loss: 6.598643  [60800/76380]
train loss: 8.931252  [64000/76380]
train loss: 14.702708  [67200/76380]
train loss: 9.416915  [70400/76380]
train loss: 5.960093  [73600/76380]
Avg validation loss: 7.820507
Epoch 4
-------------------------------
train loss: 6.003675  [    0/76380]
train loss: 8.311625  [ 3200/76380]
train loss: 6.390783  [ 6400/76380]
train loss: 6.944695  [ 9600/76380]
train loss: 6.871950  [12800/76380]
train loss: 3.513850  [16000/76380]
train loss: 10.479637  [19200/76380]
train loss: 5.314064  [22400/76380]
train loss: 10.970060  [25600/76380]
train loss: 7.663331  [28800/76380]
train loss: 9.842601  [32000/76380]
train loss: 7.184585  [35200/76380]
train loss: 9.443190  [38400/76380]
train loss: 13.147084  [41600/76380]
train loss: 3.993554  [44800/76380]
train loss: 11.680090  [48000/76380]
train loss: 6.683404  [51200/76380]
train loss: 6.108335  [54400/76380]
train loss: 5.222692  [57600/76380]
train loss: 15.391817  [60800/76380]
train loss: 4.037553  [64000/76380]
train loss: 3.745741  [67200/76380]
train loss: 9.788383  [70400/76380]
train loss: 16.098576  [73600/76380]
Avg validation loss: 7.654574
Epoch 5
-------------------------------
train loss: 8.478282  [    0/76380]
train loss: 6.257326  [ 3200/76380]
train loss: 3.387372  [ 6400/76380]
train loss: 5.757028  [ 9600/76380]
train loss: 7.697956  [12800/76380]
train loss: 6.420094  [16000/76380]
train loss: 5.077943  [19200/76380]
train loss: 3.631963  [22400/76380]
train loss: 8.802884  [25600/76380]
train loss: 4.498452  [28800/76380]
train loss: 15.830072  [32000/76380]
train loss: 4.759289  [35200/76380]
train loss: 12.120214  [38400/76380]
train loss: 3.418251  [41600/76380]
train loss: 7.193859  [44800/76380]
train loss: 10.504967  [48000/76380]
train loss: 3.282740  [51200/76380]
train loss: 11.649118  [54400/76380]
train loss: 5.478507  [57600/76380]
train loss: 5.143519  [60800/76380]
train loss: 4.996385  [64000/76380]
train loss: 5.443269  [67200/76380]
train loss: 7.211801  [70400/76380]
train loss: 6.574277  [73600/76380]
Avg validation loss: 5.980013
Epoch 6
-------------------------------
train loss: 5.582482  [    0/76380]
train loss: 6.468698  [ 3200/76380]
train loss: 5.012989  [ 6400/76380]
train loss: 10.972960  [ 9600/76380]
train loss: 4.317283  [12800/76380]
train loss: 5.486957  [16000/76380]
train loss: 9.095586  [19200/76380]
train loss: 19.305775  [22400/76380]
train loss: 2.863467  [25600/76380]
train loss: 7.398296  [28800/76380]
train loss: 25.760418  [32000/76380]
train loss: 4.947070  [35200/76380]
train loss: 5.878624  [38400/76380]
train loss: 8.831242  [41600/76380]
train loss: 7.715414  [44800/76380]
train loss: 5.189774  [48000/76380]
train loss: 5.553857  [51200/76380]
train loss: 7.208317  [54400/76380]
train loss: 9.342657  [57600/76380]
train loss: 6.103096  [60800/76380]
train loss: 10.765830  [64000/76380]
train loss: 4.158866  [67200/76380]
train loss: 6.840118  [70400/76380]
train loss: 3.969984  [73600/76380]
Avg validation loss: 5.417892
Epoch 7
-------------------------------
train loss: 6.618896  [    0/76380]
train loss: 11.817831  [ 3200/76380]
train loss: 5.318120  [ 6400/76380]
train loss: 12.481077  [ 9600/76380]
train loss: 3.516377  [12800/76380]
train loss: 2.598749  [16000/76380]
train loss: 6.032985  [19200/76380]
train loss: 8.419274  [22400/76380]
train loss: 8.354914  [25600/76380]
train loss: 4.173676  [28800/76380]
train loss: 3.158885  [32000/76380]
train loss: 4.062972  [35200/76380]
train loss: 5.278646  [38400/76380]
train loss: 9.076910  [41600/76380]
train loss: 3.070501  [44800/76380]
train loss: 3.651591  [48000/76380]
train loss: 3.793046  [51200/76380]
train loss: 18.037584  [54400/76380]
train loss: 5.931880  [57600/76380]
train loss: 6.107136  [60800/76380]
train loss: 3.151864  [64000/76380]
train loss: 5.636344  [67200/76380]
train loss: 4.194257  [70400/76380]
train loss: 4.283751  [73600/76380]
Avg validation loss: 5.867965
Epoch 8
-------------------------------
train loss: 3.748557  [    0/76380]
train loss: 4.782759  [ 3200/76380]
train loss: 6.666467  [ 6400/76380]
train loss: 3.611814  [ 9600/76380]
train loss: 4.324308  [12800/76380]
train loss: 4.483760  [16000/76380]
train loss: 4.787732  [19200/76380]
train loss: 3.697884  [22400/76380]
train loss: 5.082822  [25600/76380]
train loss: 12.870749  [28800/76380]
train loss: 5.870313  [32000/76380]
train loss: 13.133158  [35200/76380]
train loss: 9.611872  [38400/76380]
train loss: 7.557347  [41600/76380]
train loss: 3.206367  [44800/76380]
train loss: 5.140496  [48000/76380]
train loss: 10.264563  [51200/76380]
train loss: 5.273583  [54400/76380]
train loss: 6.896204  [57600/76380]
train loss: 3.480618  [60800/76380]
train loss: 2.879196  [64000/76380]
train loss: 3.892880  [67200/76380]
train loss: 8.869675  [70400/76380]
train loss: 6.330518  [73600/76380]
Avg validation loss: 4.699622
Epoch 9
-------------------------------
train loss: 9.525310  [    0/76380]
train loss: 3.159330  [ 3200/76380]
train loss: 8.893043  [ 6400/76380]
train loss: 9.812449  [ 9600/76380]
train loss: 3.171564  [12800/76380]
train loss: 15.064606  [16000/76380]
train loss: 3.423127  [19200/76380]
train loss: 3.266850  [22400/76380]
train loss: 2.570227  [25600/76380]
train loss: 7.110335  [28800/76380]
train loss: 3.700692  [32000/76380]
train loss: 9.155303  [35200/76380]
train loss: 2.814103  [38400/76380]
train loss: 8.371798  [41600/76380]
train loss: 7.479503  [44800/76380]
train loss: 3.226857  [48000/76380]
train loss: 9.979842  [51200/76380]
train loss: 9.993429  [54400/76380]
train loss: 13.637850  [57600/76380]
train loss: 4.439926  [60800/76380]
train loss: 4.671193  [64000/76380]
train loss: 4.794560  [67200/76380]
train loss: 7.118743  [70400/76380]
train loss: 6.634528  [73600/76380]
Avg validation loss: 5.721965
Epoch 10
-------------------------------
train loss: 4.090047  [    0/76380]
train loss: 4.612714  [ 3200/76380]
train loss: 3.772524  [ 6400/76380]
train loss: 2.767391  [ 9600/76380]
train loss: 5.947158  [12800/76380]
train loss: 3.666390  [16000/76380]
train loss: 3.676933  [19200/76380]
train loss: 2.557930  [22400/76380]
train loss: 4.064734  [25600/76380]
train loss: 5.240663  [28800/76380]
train loss: 4.818387  [32000/76380]
train loss: 6.955029  [35200/76380]
train loss: 15.351494  [38400/76380]
train loss: 17.828892  [41600/76380]
train loss: 8.140881  [44800/76380]
train loss: 4.313179  [48000/76380]
train loss: 4.662628  [51200/76380]
train loss: 5.761002  [54400/76380]
train loss: 4.132209  [57600/76380]
train loss: 10.188484  [60800/76380]
train loss: 4.506972  [64000/76380]
train loss: 4.375325  [67200/76380]
train loss: 4.145319  [70400/76380]
train loss: 4.037333  [73600/76380]
Avg validation loss: 4.292573
Done!