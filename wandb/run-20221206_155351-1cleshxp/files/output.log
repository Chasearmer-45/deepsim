Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 6403.765625  [    0/107449]
train loss: 95.865677  [ 3200/107449]
train loss: 48.684711  [ 6400/107449]
train loss: 104.947403  [ 9600/107449]
train loss: 35.739357  [12800/107449]
train loss: 45.088905  [16000/107449]
train loss: 86.271408  [19200/107449]
train loss: 43.262226  [22400/107449]
train loss: 43.594807  [25600/107449]
train loss: 36.090023  [28800/107449]
train loss: 144.432098  [32000/107449]
train loss: 79.767517  [35200/107449]
train loss: 61.896584  [38400/107449]
train loss: 31.616779  [41600/107449]
train loss: 57.152760  [44800/107449]
train loss: 39.795395  [48000/107449]
train loss: 76.713257  [51200/107449]
train loss: 58.107567  [54400/107449]
train loss: 88.174820  [57600/107449]
train loss: 48.420559  [60800/107449]
train loss: 73.683739  [64000/107449]
train loss: 43.667263  [67200/107449]
train loss: 62.905365  [70400/107449]
train loss: 44.725609  [73600/107449]
train loss: 30.198559  [76800/107449]
train loss: 45.344624  [80000/107449]
train loss: 39.596886  [83200/107449]
train loss: 32.398769  [86400/107449]
train loss: 44.563545  [89600/107449]
train loss: 41.519249  [92800/107449]
train loss: 41.533028  [96000/107449]
train loss: 58.041901  [99200/107449]
train loss: 48.652596  [102400/107449]
train loss: 40.088135  [105600/107449]
Avg validation loss: 43.995965
Avg out-of-distribution loss: 42.142070
Epoch 2
-------------------------------
train loss: 34.207664  [    0/107449]
train loss: 39.518333  [ 3200/107449]
train loss: 43.704544  [ 6400/107449]
train loss: 40.450680  [ 9600/107449]
train loss: 24.354898  [12800/107449]
train loss: 44.275444  [16000/107449]
train loss: 37.534061  [19200/107449]
train loss: 40.331017  [22400/107449]
train loss: 38.866230  [25600/107449]
train loss: 27.885447  [28800/107449]
train loss: 62.630043  [32000/107449]
train loss: 23.951254  [35200/107449]
train loss: 21.670265  [38400/107449]
train loss: 33.691906  [41600/107449]
train loss: 27.061184  [44800/107449]
train loss: 38.305023  [48000/107449]
train loss: 33.789135  [51200/107449]
train loss: 24.484823  [54400/107449]
train loss: 22.472809  [57600/107449]
train loss: 17.640650  [60800/107449]
train loss: 26.676640  [64000/107449]
train loss: 39.110893  [67200/107449]
train loss: 28.723789  [70400/107449]
train loss: 42.467232  [73600/107449]
train loss: 31.122763  [76800/107449]
train loss: 55.963604  [80000/107449]
train loss: 19.334650  [83200/107449]
train loss: 28.819275  [86400/107449]
train loss: 26.751486  [89600/107449]
train loss: 27.717396  [92800/107449]
train loss: 16.148071  [96000/107449]
train loss: 28.696676  [99200/107449]
train loss: 31.877142  [102400/107449]
train loss: 19.582386  [105600/107449]
Avg validation loss: 27.325142
Avg out-of-distribution loss: 28.518850
Epoch 3
-------------------------------
train loss: 13.517633  [    0/107449]
train loss: 23.731321  [ 3200/107449]
train loss: 23.712353  [ 6400/107449]
train loss: 28.457199  [ 9600/107449]
train loss: 36.913452  [12800/107449]
train loss: 19.987040  [16000/107449]
train loss: 21.930319  [19200/107449]
train loss: 27.653912  [22400/107449]
train loss: 18.357981  [25600/107449]
train loss: 26.150913  [28800/107449]
train loss: 13.923981  [32000/107449]
train loss: 17.285597  [35200/107449]
train loss: 17.474100  [38400/107449]
train loss: 17.903086  [41600/107449]
train loss: 29.769917  [44800/107449]
train loss: 21.345325  [48000/107449]
train loss: 23.725277  [51200/107449]
train loss: 24.152239  [54400/107449]
train loss: 48.759735  [57600/107449]
train loss: 32.522282  [60800/107449]
train loss: 24.817238  [64000/107449]
train loss: 18.476374  [67200/107449]
train loss: 34.319859  [70400/107449]
train loss: 16.236336  [73600/107449]
train loss: 22.481876  [76800/107449]
train loss: 22.515377  [80000/107449]
train loss: 17.005697  [83200/107449]
train loss: 14.766785  [86400/107449]
train loss: 33.598198  [89600/107449]
train loss: 40.372032  [92800/107449]
train loss: 24.411285  [96000/107449]
train loss: 23.248835  [99200/107449]
train loss: 40.568752  [102400/107449]
train loss: 18.812109  [105600/107449]
Avg validation loss: 19.945839
Avg out-of-distribution loss: 22.283086
Epoch 4
-------------------------------
train loss: 14.647963  [    0/107449]
train loss: 20.013592  [ 3200/107449]
train loss: 19.704453  [ 6400/107449]
train loss: 25.543333  [ 9600/107449]
train loss: 22.257090  [12800/107449]
train loss: 15.889246  [16000/107449]
train loss: 17.554714  [19200/107449]
train loss: 16.351456  [22400/107449]
train loss: 19.879719  [25600/107449]
train loss: 21.129126  [28800/107449]
train loss: 15.693754  [32000/107449]
train loss: 30.827103  [35200/107449]
train loss: 23.263220  [38400/107449]
train loss: 18.561359  [41600/107449]
train loss: 23.820511  [44800/107449]
train loss: 17.733868  [48000/107449]
train loss: 14.898305  [51200/107449]
train loss: 23.540180  [54400/107449]
train loss: 13.391085  [57600/107449]
train loss: 13.522049  [60800/107449]
train loss: 18.254768  [64000/107449]
train loss: 13.468013  [67200/107449]
train loss: 30.680637  [70400/107449]
train loss: 30.742228  [73600/107449]
train loss: 13.473892  [76800/107449]
train loss: 20.505613  [80000/107449]
train loss: 15.651695  [83200/107449]
train loss: 27.106558  [86400/107449]
train loss: 27.590649  [89600/107449]
train loss: 25.657669  [92800/107449]
train loss: 16.853146  [96000/107449]
train loss: 19.752575  [99200/107449]
train loss: 34.123077  [102400/107449]
train loss: 17.158270  [105600/107449]
Avg validation loss: 19.984455
Avg out-of-distribution loss: 22.426470
Epoch 5
-------------------------------
train loss: 19.310078  [    0/107449]
train loss: 9.082512  [ 3200/107449]
train loss: 10.713038  [ 6400/107449]
train loss: 16.398405  [ 9600/107449]
train loss: 9.921190  [12800/107449]
train loss: 13.651564  [16000/107449]
train loss: 19.869215  [19200/107449]
train loss: 30.976511  [22400/107449]
train loss: 14.011279  [25600/107449]
train loss: 8.303788  [28800/107449]
train loss: 13.776777  [32000/107449]
train loss: 14.138562  [35200/107449]
train loss: 13.056918  [38400/107449]
train loss: 21.708548  [41600/107449]
train loss: 18.667206  [44800/107449]
train loss: 29.523701  [48000/107449]
train loss: 13.237631  [51200/107449]
train loss: 9.624530  [54400/107449]
train loss: 18.601473  [57600/107449]
train loss: 14.421041  [60800/107449]
train loss: 17.452103  [64000/107449]
train loss: 13.162157  [67200/107449]
train loss: 19.577967  [70400/107449]
train loss: 12.197175  [73600/107449]
train loss: 25.783360  [76800/107449]
train loss: 15.820845  [80000/107449]
train loss: 16.676311  [83200/107449]
train loss: 15.653200  [86400/107449]
train loss: 11.165110  [89600/107449]
train loss: 12.023260  [92800/107449]
train loss: 14.781690  [96000/107449]
train loss: 15.787679  [99200/107449]
train loss: 12.459709  [102400/107449]
train loss: 13.458188  [105600/107449]
Avg validation loss: 16.187488
Avg out-of-distribution loss: 19.475685
Epoch 6
-------------------------------
train loss: 27.491772  [    0/107449]
train loss: 26.775814  [ 3200/107449]
train loss: 12.214632  [ 6400/107449]
train loss: 13.957698  [ 9600/107449]
train loss: 16.912159  [12800/107449]
train loss: 23.142437  [16000/107449]
train loss: 18.302914  [19200/107449]
train loss: 13.225430  [22400/107449]
train loss: 15.200740  [25600/107449]
train loss: 14.232485  [28800/107449]
train loss: 14.939115  [32000/107449]
train loss: 15.594589  [35200/107449]
train loss: 14.730766  [38400/107449]
train loss: 17.379383  [41600/107449]
train loss: 17.739773  [44800/107449]
train loss: 18.094234  [48000/107449]
train loss: 14.221689  [51200/107449]
train loss: 21.923258  [54400/107449]
train loss: 16.085867  [57600/107449]
train loss: 11.841942  [60800/107449]
train loss: 13.146602  [64000/107449]
train loss: 15.475365  [67200/107449]
train loss: 8.295597  [70400/107449]
train loss: 10.677698  [73600/107449]
train loss: 19.694576  [76800/107449]
train loss: 14.983111  [80000/107449]
train loss: 19.283770  [83200/107449]
train loss: 11.256063  [86400/107449]
train loss: 12.856474  [89600/107449]
train loss: 12.463137  [92800/107449]
train loss: 15.473215  [96000/107449]
train loss: 23.700686  [99200/107449]
train loss: 13.876714  [102400/107449]
train loss: 13.002919  [105600/107449]
Avg validation loss: 13.910677
Avg out-of-distribution loss: 17.681658
Epoch 7
-------------------------------
train loss: 15.674825  [    0/107449]
train loss: 11.743750  [ 3200/107449]
train loss: 15.030467  [ 6400/107449]
train loss: 20.207592  [ 9600/107449]
train loss: 11.759066  [12800/107449]
train loss: 18.817131  [16000/107449]
train loss: 16.064568  [19200/107449]
train loss: 14.230245  [22400/107449]
train loss: 14.319611  [25600/107449]
train loss: 18.428732  [28800/107449]
train loss: 14.832640  [32000/107449]
train loss: 9.311658  [35200/107449]
train loss: 12.412992  [38400/107449]
train loss: 7.651247  [41600/107449]
train loss: 11.127512  [44800/107449]
train loss: 11.575853  [48000/107449]
train loss: 12.684000  [51200/107449]
train loss: 17.182590  [54400/107449]
train loss: 31.053354  [57600/107449]
train loss: 10.088254  [60800/107449]
train loss: 13.518562  [64000/107449]
train loss: 14.223526  [67200/107449]
train loss: 11.820108  [70400/107449]
train loss: 12.186112  [73600/107449]
train loss: 9.088371  [76800/107449]
train loss: 12.371001  [80000/107449]
train loss: 15.378886  [83200/107449]
train loss: 19.309582  [86400/107449]
train loss: 13.514211  [89600/107449]
train loss: 9.766157  [92800/107449]
train loss: 16.119755  [96000/107449]
train loss: 8.890468  [99200/107449]
train loss: 10.865600  [102400/107449]
train loss: 13.610982  [105600/107449]
Avg validation loss: 14.068791
Avg out-of-distribution loss: 16.832831
Epoch 8
-------------------------------
train loss: 12.383770  [    0/107449]
train loss: 11.301305  [ 3200/107449]
train loss: 13.230722  [ 6400/107449]
train loss: 15.419031  [ 9600/107449]
train loss: 12.450852  [12800/107449]
train loss: 19.271057  [16000/107449]
train loss: 10.234716  [19200/107449]
train loss: 14.427170  [22400/107449]
train loss: 9.912517  [25600/107449]
train loss: 19.442774  [28800/107449]
train loss: 11.073743  [32000/107449]
train loss: 10.914936  [35200/107449]
train loss: 15.004250  [38400/107449]
train loss: 8.192926  [41600/107449]
train loss: 10.732697  [44800/107449]
train loss: 10.252696  [48000/107449]
train loss: 11.054281  [51200/107449]
train loss: 17.947140  [54400/107449]
train loss: 17.344501  [57600/107449]
train loss: 10.009521  [60800/107449]
train loss: 14.656828  [64000/107449]
train loss: 10.295452  [67200/107449]
train loss: 11.360864  [70400/107449]
train loss: 11.165677  [73600/107449]
train loss: 11.926870  [76800/107449]
train loss: 8.568376  [80000/107449]
train loss: 13.629809  [83200/107449]
train loss: 9.962281  [86400/107449]
train loss: 12.126123  [89600/107449]
train loss: 10.436575  [92800/107449]
train loss: 12.003851  [96000/107449]
train loss: 9.739718  [99200/107449]
train loss: 14.696569  [102400/107449]
train loss: 19.953121  [105600/107449]
Avg validation loss: 11.318396
Avg out-of-distribution loss: 14.683695
Epoch 9
-------------------------------
train loss: 21.146168  [    0/107449]
train loss: 11.038245  [ 3200/107449]
train loss: 8.816920  [ 6400/107449]
train loss: 15.209882  [ 9600/107449]
train loss: 8.819155  [12800/107449]
train loss: 15.694426  [16000/107449]
train loss: 11.012812  [19200/107449]
train loss: 13.775671  [22400/107449]
train loss: 9.110553  [25600/107449]
train loss: 13.670255  [28800/107449]
train loss: 11.512274  [32000/107449]
train loss: 12.359861  [35200/107449]
train loss: 9.803315  [38400/107449]
train loss: 11.841136  [41600/107449]
train loss: 8.946924  [44800/107449]
train loss: 10.649977  [48000/107449]
train loss: 10.103203  [51200/107449]
train loss: 8.019685  [54400/107449]
train loss: 9.137166  [57600/107449]
train loss: 12.325937  [60800/107449]
train loss: 10.869493  [64000/107449]
train loss: 10.235601  [67200/107449]
train loss: 11.440932  [70400/107449]
train loss: 11.199264  [73600/107449]
train loss: 15.333282  [76800/107449]
train loss: 12.694392  [80000/107449]
train loss: 11.436177  [83200/107449]
train loss: 7.520308  [86400/107449]
train loss: 9.581021  [89600/107449]
train loss: 10.440858  [92800/107449]
train loss: 12.708166  [96000/107449]
train loss: 12.606217  [99200/107449]
train loss: 21.420284  [102400/107449]
train loss: 6.972821  [105600/107449]
Avg validation loss: 11.254883
Avg out-of-distribution loss: 15.409429
Epoch 10
-------------------------------
train loss: 11.271533  [    0/107449]
train loss: 20.569971  [ 3200/107449]
train loss: 10.106479  [ 6400/107449]
train loss: 8.250151  [ 9600/107449]
train loss: 9.041771  [12800/107449]
train loss: 10.524528  [16000/107449]
train loss: 10.717235  [19200/107449]
train loss: 10.610723  [22400/107449]
train loss: 9.952887  [25600/107449]
train loss: 14.337506  [28800/107449]
train loss: 10.375360  [32000/107449]
train loss: 9.464167  [35200/107449]
train loss: 14.318486  [38400/107449]
train loss: 17.544748  [41600/107449]
train loss: 11.839067  [44800/107449]
train loss: 9.570495  [48000/107449]
train loss: 10.350114  [51200/107449]
train loss: 14.705400  [54400/107449]
train loss: 11.187367  [57600/107449]
train loss: 9.898068  [60800/107449]
train loss: 11.001043  [64000/107449]
train loss: 13.132851  [67200/107449]
train loss: 12.110192  [70400/107449]
train loss: 6.516753  [73600/107449]
train loss: 13.294141  [76800/107449]
train loss: 12.302709  [80000/107449]
train loss: 5.599583  [83200/107449]
train loss: 9.268705  [86400/107449]
train loss: 9.291056  [89600/107449]
train loss: 7.232057  [92800/107449]
train loss: 10.422717  [96000/107449]
train loss: 8.547400  [99200/107449]
train loss: 8.718754  [102400/107449]
train loss: 13.145020  [105600/107449]
Avg validation loss: 11.286080
Avg out-of-distribution loss: 15.004737
Epoch 11
-------------------------------
train loss: 9.653311  [    0/107449]
train loss: 7.964835  [ 3200/107449]
train loss: 12.248613  [ 6400/107449]
train loss: 14.062106  [ 9600/107449]
train loss: 6.619576  [12800/107449]
train loss: 8.756501  [16000/107449]
train loss: 7.368219  [19200/107449]
train loss: 10.482518  [22400/107449]
train loss: 12.373978  [25600/107449]
train loss: 15.539597  [28800/107449]
train loss: 9.414485  [32000/107449]
train loss: 9.968378  [35200/107449]
train loss: 10.240468  [38400/107449]
train loss: 7.370257  [41600/107449]
train loss: 12.238679  [44800/107449]
train loss: 9.427301  [48000/107449]
train loss: 7.661075  [51200/107449]
train loss: 6.756663  [54400/107449]
train loss: 10.544960  [57600/107449]
train loss: 8.822339  [60800/107449]
train loss: 9.241761  [64000/107449]
train loss: 7.223218  [67200/107449]
train loss: 11.675007  [70400/107449]
train loss: 12.282288  [73600/107449]
train loss: 5.883219  [76800/107449]
train loss: 12.357722  [80000/107449]
train loss: 7.808038  [83200/107449]
train loss: 9.493246  [86400/107449]
train loss: 7.187561  [89600/107449]
train loss: 13.998998  [92800/107449]
train loss: 10.555619  [96000/107449]
train loss: 5.819435  [99200/107449]
train loss: 15.721764  [102400/107449]
train loss: 10.935063  [105600/107449]
Avg validation loss: 9.601483
Avg out-of-distribution loss: 14.139696
Epoch 12
-------------------------------
train loss: 23.017450  [    0/107449]
train loss: 11.216522  [ 3200/107449]
train loss: 13.182935  [ 6400/107449]
train loss: 9.927760  [ 9600/107449]
train loss: 5.128022  [12800/107449]
train loss: 10.775344  [16000/107449]
train loss: 7.622985  [19200/107449]
train loss: 13.504100  [22400/107449]
train loss: 9.105851  [25600/107449]
train loss: 9.681381  [28800/107449]
train loss: 5.850109  [32000/107449]
train loss: 9.935741  [35200/107449]
train loss: 9.380678  [38400/107449]
train loss: 8.023848  [41600/107449]
train loss: 7.505049  [44800/107449]
train loss: 10.172173  [48000/107449]
train loss: 9.364965  [51200/107449]
train loss: 8.728970  [54400/107449]
train loss: 8.142751  [57600/107449]
train loss: 5.722117  [60800/107449]
train loss: 8.948004  [64000/107449]
train loss: 10.739881  [67200/107449]
train loss: 8.548216  [70400/107449]
train loss: 9.859690  [73600/107449]
train loss: 14.355665  [76800/107449]
train loss: 10.358641  [80000/107449]
train loss: 9.051628  [83200/107449]
train loss: 11.294848  [86400/107449]
train loss: 29.123596  [89600/107449]
train loss: 7.069067  [92800/107449]
train loss: 7.209946  [96000/107449]
train loss: 15.136581  [99200/107449]
train loss: 13.813787  [102400/107449]
train loss: 7.192510  [105600/107449]
Avg validation loss: 10.136237
Avg out-of-distribution loss: 14.477910
Epoch 13
-------------------------------
train loss: 11.759981  [    0/107449]
train loss: 8.288991  [ 3200/107449]
train loss: 7.997885  [ 6400/107449]
train loss: 7.320563  [ 9600/107449]
train loss: 5.934502  [12800/107449]
train loss: 6.942832  [16000/107449]
train loss: 7.432541  [19200/107449]
train loss: 7.962164  [22400/107449]
train loss: 10.041900  [25600/107449]
train loss: 9.358432  [28800/107449]
train loss: 6.760097  [32000/107449]
train loss: 8.960361  [35200/107449]
train loss: 7.315413  [38400/107449]
train loss: 17.920389  [41600/107449]
train loss: 7.747996  [44800/107449]
train loss: 9.912934  [48000/107449]
train loss: 6.867663  [51200/107449]
train loss: 10.011580  [54400/107449]
train loss: 10.021017  [57600/107449]
train loss: 16.075125  [60800/107449]
train loss: 8.890164  [64000/107449]
train loss: 8.918136  [67200/107449]
train loss: 7.476721  [70400/107449]
train loss: 11.248377  [73600/107449]
train loss: 8.790716  [76800/107449]
train loss: 13.126010  [80000/107449]
train loss: 8.970610  [83200/107449]
train loss: 7.658299  [86400/107449]
train loss: 9.576571  [89600/107449]
train loss: 9.569666  [92800/107449]
train loss: 6.407725  [96000/107449]
train loss: 12.492211  [99200/107449]
train loss: 8.216163  [102400/107449]
train loss: 9.954905  [105600/107449]
Avg validation loss: 8.739762
Avg out-of-distribution loss: 12.801839
Epoch 14
-------------------------------
train loss: 5.358991  [    0/107449]
train loss: 8.381899  [ 3200/107449]
train loss: 7.328526  [ 6400/107449]
train loss: 6.968311  [ 9600/107449]
train loss: 9.832495  [12800/107449]
train loss: 7.683825  [16000/107449]
train loss: 8.245258  [19200/107449]
train loss: 7.727394  [22400/107449]
train loss: 8.518543  [25600/107449]
train loss: 11.453751  [28800/107449]
train loss: 10.631371  [32000/107449]
train loss: 10.181587  [35200/107449]
train loss: 9.301191  [38400/107449]
train loss: 6.841751  [41600/107449]
train loss: 7.231761  [44800/107449]
train loss: 7.038439  [48000/107449]
train loss: 6.276834  [51200/107449]
train loss: 7.653397  [54400/107449]
train loss: 10.336819  [57600/107449]
train loss: 12.497113  [60800/107449]
train loss: 9.664528  [64000/107449]
train loss: 12.701551  [67200/107449]
train loss: 8.921336  [70400/107449]
train loss: 8.353785  [73600/107449]
train loss: 7.425113  [76800/107449]
train loss: 5.912845  [80000/107449]
train loss: 13.078525  [83200/107449]
train loss: 11.071277  [86400/107449]
train loss: 8.393901  [89600/107449]
train loss: 11.294881  [92800/107449]
train loss: 10.776190  [96000/107449]
train loss: 8.938070  [99200/107449]
train loss: 7.993062  [102400/107449]
train loss: 8.196647  [105600/107449]
Avg validation loss: 10.410488
Avg out-of-distribution loss: 13.719111
Epoch 15
-------------------------------
train loss: 11.332130  [    0/107449]
train loss: 11.329540  [ 3200/107449]
train loss: 12.938377  [ 6400/107449]
train loss: 16.208609  [ 9600/107449]
train loss: 12.694928  [12800/107449]
train loss: 8.736823  [16000/107449]
train loss: 9.898462  [19200/107449]
train loss: 13.079435  [22400/107449]
train loss: 8.959037  [25600/107449]
train loss: 7.352377  [28800/107449]
train loss: 8.692819  [32000/107449]
train loss: 7.365239  [35200/107449]
train loss: 7.107351  [38400/107449]
train loss: 10.196307  [41600/107449]
train loss: 10.806675  [44800/107449]
train loss: 6.291667  [48000/107449]
train loss: 7.690184  [51200/107449]
train loss: 6.816403  [54400/107449]
train loss: 8.192674  [57600/107449]
train loss: 12.657518  [60800/107449]
train loss: 7.804879  [64000/107449]
train loss: 19.122183  [67200/107449]
train loss: 5.019760  [70400/107449]
train loss: 6.826546  [73600/107449]
train loss: 7.707343  [76800/107449]
train loss: 8.329612  [80000/107449]
train loss: 6.331546  [83200/107449]
train loss: 13.856133  [86400/107449]
train loss: 4.370534  [89600/107449]
train loss: 12.306990  [92800/107449]
train loss: 6.068855  [96000/107449]
train loss: 5.432761  [99200/107449]
train loss: 8.020961  [102400/107449]
train loss: 7.879830  [105600/107449]
Avg validation loss: 9.511666
Avg out-of-distribution loss: 14.020332
Epoch 16
-------------------------------
train loss: 11.284456  [    0/107449]
train loss: 8.813406  [ 3200/107449]
train loss: 7.238830  [ 6400/107449]
train loss: 5.872469  [ 9600/107449]
train loss: 5.650544  [12800/107449]
train loss: 8.095225  [16000/107449]
train loss: 6.526266  [19200/107449]
train loss: 7.997261  [22400/107449]
train loss: 5.181742  [25600/107449]
train loss: 8.319681  [28800/107449]
train loss: 8.052118  [32000/107449]
train loss: 8.647418  [35200/107449]
train loss: 9.325846  [38400/107449]
train loss: 9.807961  [41600/107449]
train loss: 8.488432  [44800/107449]
train loss: 10.542804  [48000/107449]
train loss: 9.647489  [51200/107449]
train loss: 6.529238  [54400/107449]
train loss: 7.901144  [57600/107449]
train loss: 10.199347  [60800/107449]
train loss: 8.304219  [64000/107449]
train loss: 11.218734  [67200/107449]
train loss: 6.973572  [70400/107449]
train loss: 8.449628  [73600/107449]
train loss: 12.604204  [76800/107449]
train loss: 7.311876  [80000/107449]
train loss: 6.510635  [83200/107449]
train loss: 8.214635  [86400/107449]
train loss: 8.341710  [89600/107449]
train loss: 5.958445  [92800/107449]
train loss: 17.033703  [96000/107449]
train loss: 8.322145  [99200/107449]
train loss: 7.312931  [102400/107449]
train loss: 6.470824  [105600/107449]
Avg validation loss: 8.122683
Avg out-of-distribution loss: 12.162032
Epoch 17
-------------------------------
train loss: 8.760799  [    0/107449]
train loss: 7.602407  [ 3200/107449]
train loss: 8.779821  [ 6400/107449]
train loss: 10.074879  [ 9600/107449]
train loss: 13.693636  [12800/107449]
train loss: 8.980830  [16000/107449]
train loss: 8.660482  [19200/107449]
train loss: 10.170847  [22400/107449]
train loss: 6.706968  [25600/107449]
train loss: 11.061481  [28800/107449]
train loss: 5.186858  [32000/107449]
train loss: 10.156270  [35200/107449]
train loss: 10.802567  [38400/107449]
train loss: 5.295958  [41600/107449]
train loss: 14.612799  [44800/107449]
train loss: 10.767994  [48000/107449]
train loss: 7.254563  [51200/107449]
train loss: 7.204351  [54400/107449]
train loss: 7.821710  [57600/107449]
train loss: 9.711889  [60800/107449]
train loss: 6.494078  [64000/107449]
train loss: 4.677700  [67200/107449]
train loss: 6.727754  [70400/107449]
train loss: 5.978847  [73600/107449]
train loss: 6.238001  [76800/107449]
train loss: 8.442952  [80000/107449]
train loss: 6.345315  [83200/107449]
train loss: 7.479594  [86400/107449]
train loss: 7.019639  [89600/107449]
train loss: 7.321781  [92800/107449]
train loss: 7.217510  [96000/107449]
train loss: 7.549138  [99200/107449]
train loss: 8.430825  [102400/107449]
train loss: 6.068860  [105600/107449]
Avg validation loss: 8.521150
Avg out-of-distribution loss: 12.225504
Epoch 18
-------------------------------
train loss: 5.155475  [    0/107449]
train loss: 6.413159  [ 3200/107449]
train loss: 4.542594  [ 6400/107449]
train loss: 8.537683  [ 9600/107449]
train loss: 12.556961  [12800/107449]
train loss: 8.104838  [16000/107449]
train loss: 8.164290  [19200/107449]
train loss: 5.397021  [22400/107449]
train loss: 5.159846  [25600/107449]
train loss: 6.550206  [28800/107449]
train loss: 8.598288  [32000/107449]
train loss: 7.552042  [35200/107449]
train loss: 8.156019  [38400/107449]
train loss: 8.620165  [41600/107449]
train loss: 6.430077  [44800/107449]
train loss: 6.965907  [48000/107449]
train loss: 5.488852  [51200/107449]
train loss: 5.627969  [54400/107449]
train loss: 6.771249  [57600/107449]
train loss: 10.711929  [60800/107449]
train loss: 8.836306  [64000/107449]
train loss: 6.567482  [67200/107449]
train loss: 10.204186  [70400/107449]
train loss: 11.201306  [73600/107449]
train loss: 9.698492  [76800/107449]
train loss: 7.529542  [80000/107449]
train loss: 7.422329  [83200/107449]
train loss: 18.161654  [86400/107449]
train loss: 7.113846  [89600/107449]
train loss: 6.872550  [92800/107449]
train loss: 8.735128  [96000/107449]
train loss: 7.578913  [99200/107449]
train loss: 6.036571  [102400/107449]
train loss: 10.204694  [105600/107449]
Avg validation loss: 10.041873
Avg out-of-distribution loss: 13.773697
Epoch 19
-------------------------------
train loss: 19.546778  [    0/107449]
train loss: 11.261645  [ 3200/107449]
train loss: 5.943938  [ 6400/107449]
train loss: 6.045009  [ 9600/107449]
train loss: 7.718642  [12800/107449]
train loss: 4.923408  [16000/107449]
train loss: 6.513209  [19200/107449]
train loss: 6.985489  [22400/107449]
train loss: 9.089941  [25600/107449]
train loss: 5.733316  [28800/107449]
train loss: 8.563874  [32000/107449]
train loss: 8.909631  [35200/107449]
train loss: 6.898598  [38400/107449]
train loss: 7.943872  [41600/107449]
train loss: 8.855673  [44800/107449]
train loss: 9.358943  [48000/107449]
train loss: 11.946867  [51200/107449]
train loss: 4.875882  [54400/107449]
train loss: 6.984726  [57600/107449]
train loss: 7.139524  [60800/107449]
train loss: 7.061566  [64000/107449]
train loss: 6.513009  [67200/107449]
train loss: 8.603952  [70400/107449]
train loss: 7.370547  [73600/107449]
train loss: 10.326681  [76800/107449]
train loss: 5.777505  [80000/107449]
train loss: 7.587494  [83200/107449]
train loss: 8.084129  [86400/107449]
train loss: 7.396671  [89600/107449]
train loss: 10.618868  [92800/107449]
train loss: 10.045637  [96000/107449]
train loss: 9.842856  [99200/107449]
train loss: 7.655443  [102400/107449]
train loss: 6.229095  [105600/107449]
Avg validation loss: 8.591107
Avg out-of-distribution loss: 13.078155
Epoch 20
-------------------------------
train loss: 9.830107  [    0/107449]
train loss: 9.450392  [ 3200/107449]
train loss: 6.518788  [ 6400/107449]
train loss: 6.594612  [ 9600/107449]
train loss: 7.227899  [12800/107449]
train loss: 6.766114  [16000/107449]
train loss: 8.839811  [19200/107449]
train loss: 11.537359  [22400/107449]
train loss: 7.973440  [25600/107449]
train loss: 5.042435  [28800/107449]
train loss: 6.559119  [32000/107449]
train loss: 7.393270  [35200/107449]
train loss: 7.501786  [38400/107449]
train loss: 5.548821  [41600/107449]
train loss: 10.287340  [44800/107449]
train loss: 6.629985  [48000/107449]
train loss: 8.391644  [51200/107449]
train loss: 16.474054  [54400/107449]
train loss: 8.027403  [57600/107449]
train loss: 4.881909  [60800/107449]
train loss: 9.648520  [64000/107449]
train loss: 8.893602  [67200/107449]
train loss: 7.019467  [70400/107449]
train loss: 8.319082  [73600/107449]
train loss: 6.724758  [76800/107449]
train loss: 6.949822  [80000/107449]
train loss: 9.929516  [83200/107449]
train loss: 6.739720  [86400/107449]
train loss: 7.589208  [89600/107449]
train loss: 7.059854  [92800/107449]
train loss: 9.360759  [96000/107449]
train loss: 6.583283  [99200/107449]
train loss: 8.887836  [102400/107449]
train loss: 9.959322  [105600/107449]
Avg validation loss: 7.736313
Avg out-of-distribution loss: 12.249307
Done!