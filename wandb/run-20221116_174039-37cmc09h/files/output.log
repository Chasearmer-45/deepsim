Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 3339.650391  [    0/76380]
train loss: 858.621216  [ 3200/76380]
train loss: 54.053234  [ 6400/76380]
train loss: 25.570419  [ 9600/76380]
train loss: 14.748339  [12800/76380]
train loss: 15.694422  [16000/76380]
train loss: 9.226062  [19200/76380]
train loss: 16.693062  [22400/76380]
train loss: 11.317128  [25600/76380]
train loss: 10.418766  [28800/76380]
train loss: 8.405701  [32000/76380]
train loss: 13.927088  [35200/76380]
train loss: 6.648077  [38400/76380]
train loss: 8.023841  [41600/76380]
train loss: 11.343857  [44800/76380]
train loss: 9.707522  [48000/76380]
train loss: 13.016611  [51200/76380]
train loss: 10.255152  [54400/76380]
train loss: 9.646247  [57600/76380]
train loss: 17.245472  [60800/76380]
train loss: 7.532990  [64000/76380]
train loss: 10.394008  [67200/76380]
train loss: 8.143717  [70400/76380]
train loss: 14.402861  [73600/76380]
Avg validation loss: 12.237688
Epoch 2
-------------------------------
train loss: 10.704731  [    0/76380]
train loss: 7.795149  [ 3200/76380]
train loss: 13.095200  [ 6400/76380]
train loss: 3.119453  [ 9600/76380]
train loss: 9.062330  [12800/76380]
train loss: 7.643141  [16000/76380]
train loss: 9.672986  [19200/76380]
train loss: 12.480777  [22400/76380]
train loss: 10.601524  [25600/76380]
train loss: 9.556750  [28800/76380]
train loss: 7.942231  [32000/76380]
train loss: 13.020021  [35200/76380]
train loss: 4.516512  [38400/76380]
train loss: 9.812815  [41600/76380]
train loss: 4.993524  [44800/76380]
train loss: 9.054054  [48000/76380]
train loss: 4.379274  [51200/76380]
train loss: 9.386959  [54400/76380]
train loss: 5.286084  [57600/76380]
train loss: 10.449491  [60800/76380]
train loss: 7.570011  [64000/76380]
train loss: 15.026022  [67200/76380]
train loss: 10.265546  [70400/76380]
train loss: 13.892255  [73600/76380]
Avg validation loss: 8.416967
Epoch 3
-------------------------------
train loss: 14.428846  [    0/76380]
train loss: 6.877698  [ 3200/76380]
train loss: 10.956985  [ 6400/76380]
train loss: 7.870228  [ 9600/76380]
train loss: 12.614949  [12800/76380]
train loss: 5.186143  [16000/76380]
train loss: 9.014795  [19200/76380]
train loss: 9.186129  [22400/76380]
train loss: 15.552289  [25600/76380]
train loss: 4.793988  [28800/76380]
train loss: 15.212816  [32000/76380]
train loss: 15.689726  [35200/76380]
train loss: 8.246027  [38400/76380]
train loss: 6.062250  [41600/76380]
train loss: 7.607291  [44800/76380]
train loss: 6.724627  [48000/76380]
train loss: 10.083543  [51200/76380]
train loss: 6.485713  [54400/76380]
train loss: 5.113258  [57600/76380]
train loss: 12.180650  [60800/76380]
train loss: 15.351629  [64000/76380]
train loss: 4.617085  [67200/76380]
train loss: 5.022083  [70400/76380]
train loss: 11.417505  [73600/76380]
Avg validation loss: 9.625446
Epoch 4
-------------------------------
train loss: 7.896719  [    0/76380]
train loss: 6.812678  [ 3200/76380]
train loss: 9.814960  [ 6400/76380]
train loss: 4.738664  [ 9600/76380]
train loss: 13.168010  [12800/76380]
train loss: 7.436037  [16000/76380]
train loss: 7.163235  [19200/76380]
train loss: 6.283654  [22400/76380]
train loss: 6.534818  [25600/76380]
train loss: 11.356390  [28800/76380]
train loss: 19.636425  [32000/76380]
train loss: 8.744184  [35200/76380]
train loss: 8.022746  [38400/76380]
train loss: 7.614614  [41600/76380]
train loss: 9.015890  [44800/76380]
train loss: 4.592148  [48000/76380]
train loss: 15.180304  [51200/76380]
train loss: 6.962294  [54400/76380]
train loss: 14.121529  [57600/76380]
train loss: 6.485430  [60800/76380]
train loss: 6.502608  [64000/76380]
train loss: 4.702803  [67200/76380]
train loss: 6.847859  [70400/76380]
train loss: 9.216823  [73600/76380]
Avg validation loss: 6.361866
Epoch 5
-------------------------------
train loss: 5.947100  [    0/76380]
train loss: 19.371485  [ 3200/76380]
train loss: 3.972350  [ 6400/76380]
train loss: 6.564035  [ 9600/76380]
train loss: 18.487665  [12800/76380]
train loss: 5.061250  [16000/76380]
train loss: 10.776236  [19200/76380]
train loss: 6.278254  [22400/76380]
train loss: 7.146195  [25600/76380]
train loss: 7.644951  [28800/76380]
train loss: 4.804181  [32000/76380]
train loss: 7.667396  [35200/76380]
train loss: 7.981093  [38400/76380]
train loss: 7.986373  [41600/76380]
train loss: 37.897850  [44800/76380]
train loss: 4.426255  [48000/76380]
train loss: 13.182539  [51200/76380]
train loss: 4.150686  [54400/76380]
train loss: 7.294347  [57600/76380]
train loss: 5.105446  [60800/76380]
train loss: 8.151854  [64000/76380]
train loss: 3.502593  [67200/76380]
train loss: 7.156776  [70400/76380]
train loss: 6.705742  [73600/76380]
Avg validation loss: 6.480238
Epoch 6
-------------------------------
train loss: 9.241735  [    0/76380]
train loss: 9.779406  [ 3200/76380]
train loss: 4.572598  [ 6400/76380]
train loss: 5.808700  [ 9600/76380]
train loss: 8.569704  [12800/76380]
train loss: 8.561979  [16000/76380]
train loss: 5.394482  [19200/76380]
train loss: 8.066459  [22400/76380]
train loss: 6.404270  [25600/76380]
train loss: 7.056540  [28800/76380]
train loss: 6.682791  [32000/76380]
train loss: 6.092781  [35200/76380]
train loss: 8.989500  [38400/76380]
train loss: 4.388902  [41600/76380]
train loss: 3.884569  [44800/76380]
train loss: 5.285630  [48000/76380]
train loss: 5.691942  [51200/76380]
train loss: 6.809279  [54400/76380]
train loss: 7.318307  [57600/76380]
train loss: 6.771451  [60800/76380]
train loss: 6.285262  [64000/76380]
train loss: 5.094201  [67200/76380]
train loss: 21.414955  [70400/76380]
train loss: 6.674090  [73600/76380]
Avg validation loss: 5.831658
Epoch 7
-------------------------------
train loss: 9.281912  [    0/76380]
train loss: 32.233685  [ 3200/76380]
train loss: 6.771456  [ 6400/76380]
train loss: 4.913136  [ 9600/76380]
train loss: 11.929018  [12800/76380]
train loss: 5.871353  [16000/76380]
train loss: 5.741021  [19200/76380]
train loss: 8.356436  [22400/76380]
train loss: 6.420794  [25600/76380]
train loss: 8.412585  [28800/76380]
train loss: 4.481289  [32000/76380]
train loss: 5.843226  [35200/76380]
train loss: 6.806023  [38400/76380]
train loss: 9.544265  [41600/76380]
train loss: 6.403717  [44800/76380]
train loss: 11.646680  [48000/76380]
train loss: 4.776493  [51200/76380]
train loss: 8.814233  [54400/76380]
train loss: 5.453487  [57600/76380]
train loss: 3.709953  [60800/76380]
train loss: 7.413924  [64000/76380]
train loss: 3.442682  [67200/76380]
train loss: 70.273193  [70400/76380]
train loss: 6.420208  [73600/76380]
Avg validation loss: 5.436722
Epoch 8
-------------------------------
train loss: 3.955961  [    0/76380]
train loss: 3.467476  [ 3200/76380]
train loss: 6.371739  [ 6400/76380]
train loss: 4.436407  [ 9600/76380]
train loss: 5.919938  [12800/76380]
train loss: 3.123727  [16000/76380]
train loss: 6.839337  [19200/76380]
train loss: 7.182260  [22400/76380]
train loss: 7.602064  [25600/76380]
train loss: 9.863887  [28800/76380]
train loss: 11.748125  [32000/76380]
train loss: 8.041815  [35200/76380]
train loss: 3.851395  [38400/76380]
train loss: 7.151998  [41600/76380]
train loss: 16.528275  [44800/76380]
train loss: 4.699135  [48000/76380]
train loss: 7.774831  [51200/76380]
train loss: 7.188226  [54400/76380]
train loss: 4.173680  [57600/76380]
train loss: 7.507107  [60800/76380]
train loss: 5.238005  [64000/76380]
train loss: 10.223024  [67200/76380]
train loss: 3.868722  [70400/76380]
train loss: 5.741521  [73600/76380]
Avg validation loss: 6.026613
Epoch 9
-------------------------------
train loss: 5.111698  [    0/76380]
train loss: 4.330278  [ 3200/76380]
train loss: 6.818954  [ 6400/76380]
train loss: 4.329547  [ 9600/76380]
train loss: 5.516002  [12800/76380]
train loss: 7.496478  [16000/76380]
train loss: 3.098925  [19200/76380]
train loss: 9.652237  [22400/76380]
train loss: 12.490719  [25600/76380]
train loss: 4.908051  [28800/76380]
train loss: 8.916100  [32000/76380]
train loss: 8.448114  [35200/76380]
train loss: 5.670413  [38400/76380]
train loss: 6.951741  [41600/76380]
train loss: 5.717874  [44800/76380]
train loss: 9.058078  [48000/76380]
train loss: 7.547904  [51200/76380]
train loss: 5.774424  [54400/76380]
train loss: 23.369514  [57600/76380]
train loss: 20.712282  [60800/76380]
train loss: 4.476728  [64000/76380]
train loss: 9.346326  [67200/76380]
train loss: 4.466946  [70400/76380]
train loss: 8.960726  [73600/76380]
Avg validation loss: 5.226361
Epoch 10
-------------------------------
train loss: 3.734233  [    0/76380]
train loss: 5.491482  [ 3200/76380]
train loss: 3.163223  [ 6400/76380]
train loss: 5.256678  [ 9600/76380]
train loss: 4.070298  [12800/76380]
train loss: 8.541994  [16000/76380]
train loss: 2.777614  [19200/76380]
train loss: 6.247580  [22400/76380]
train loss: 6.247209  [25600/76380]
train loss: 5.525232  [28800/76380]
train loss: 3.725890  [32000/76380]
train loss: 2.484043  [35200/76380]
train loss: 5.826991  [38400/76380]
train loss: 6.409311  [41600/76380]
train loss: 4.158974  [44800/76380]
train loss: 4.674154  [48000/76380]
train loss: 3.682799  [51200/76380]
train loss: 8.385193  [54400/76380]
train loss: 3.675739  [57600/76380]
train loss: 4.703252  [60800/76380]
train loss: 3.350860  [64000/76380]
train loss: 6.170966  [67200/76380]
train loss: 4.305164  [70400/76380]
train loss: 4.843305  [73600/76380]
Avg validation loss: 16.806918
Done!