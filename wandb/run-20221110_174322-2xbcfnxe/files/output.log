Shape of X: torch.Size([32, 31])
Shape of y: torch.Size([32, 5]) torch.float32
Using cpu device
NeuralNetwork(
  (linear_relu_stack): Sequential(
    (0): Linear(in_features=31, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): ReLU()
    (4): Linear(in_features=64, out_features=64, bias=True)
    (5): ReLU()
    (6): Linear(in_features=64, out_features=64, bias=True)
    (7): ReLU()
    (8): Linear(in_features=64, out_features=5, bias=True)
  )
)
Epoch 1
-------------------------------
train loss: 11147.833984  [    0/76380]
train loss: 1616.485962  [ 3200/76380]
train loss: 238.380569  [ 6400/76380]
train loss: 60.405312  [ 9600/76380]
train loss: 16.286865  [12800/76380]
train loss: 17.860373  [16000/76380]
train loss: 11.082122  [19200/76380]
train loss: 8.027906  [22400/76380]
train loss: 8.623253  [25600/76380]
train loss: 10.343325  [28800/76380]
train loss: 14.350291  [32000/76380]
train loss: 13.033399  [35200/76380]
train loss: 8.513163  [38400/76380]
train loss: 13.024526  [41600/76380]
train loss: 13.468588  [44800/76380]
train loss: 12.512511  [48000/76380]
train loss: 10.569021  [51200/76380]
train loss: 13.186023  [54400/76380]
train loss: 5.567317  [57600/76380]
train loss: 4.378362  [60800/76380]
train loss: 5.311505  [64000/76380]
train loss: 10.375023  [67200/76380]
train loss: 7.377945  [70400/76380]
train loss: 8.327902  [73600/76380]
Avg validation loss: 12.631650
Epoch 2
-------------------------------
train loss: 7.377950  [    0/76380]
train loss: 7.558440  [ 3200/76380]
train loss: 5.802934  [ 6400/76380]
train loss: 4.713146  [ 9600/76380]
train loss: 7.026088  [12800/76380]
train loss: 5.637507  [16000/76380]
train loss: 15.594554  [19200/76380]
train loss: 8.708799  [22400/76380]
train loss: 16.795940  [25600/76380]
train loss: 7.267186  [28800/76380]
train loss: 10.507288  [32000/76380]
train loss: 4.192207  [35200/76380]
train loss: 6.949337  [38400/76380]
train loss: 10.324106  [41600/76380]
train loss: 9.338099  [44800/76380]
train loss: 6.655562  [48000/76380]
train loss: 3.627291  [51200/76380]
train loss: 9.003607  [54400/76380]
train loss: 3.304864  [57600/76380]
train loss: 8.679095  [60800/76380]
train loss: 19.309505  [64000/76380]
train loss: 6.081790  [67200/76380]
train loss: 17.071901  [70400/76380]
train loss: 3.725065  [73600/76380]
Avg validation loss: 7.283752
Epoch 3
-------------------------------
train loss: 7.547559  [    0/76380]
train loss: 4.344632  [ 3200/76380]
train loss: 17.384266  [ 6400/76380]
train loss: 8.506252  [ 9600/76380]
train loss: 10.448192  [12800/76380]
train loss: 7.630594  [16000/76380]
train loss: 9.357942  [19200/76380]
train loss: 2.969329  [22400/76380]
train loss: 7.056726  [25600/76380]
train loss: 4.818799  [28800/76380]
train loss: 9.996053  [32000/76380]
train loss: 5.680123  [35200/76380]
train loss: 8.635160  [38400/76380]
train loss: 3.408977  [41600/76380]
train loss: 5.365858  [44800/76380]
train loss: 6.702319  [48000/76380]
train loss: 9.966625  [51200/76380]
train loss: 3.994982  [54400/76380]
train loss: 11.863605  [57600/76380]
train loss: 6.026482  [60800/76380]
train loss: 17.054844  [64000/76380]
train loss: 6.741868  [67200/76380]
train loss: 8.310454  [70400/76380]
train loss: 7.160476  [73600/76380]
Avg validation loss: 7.151881
Epoch 4
-------------------------------
train loss: 5.663129  [    0/76380]
train loss: 6.671715  [ 3200/76380]
train loss: 20.745876  [ 6400/76380]
train loss: 13.027206  [ 9600/76380]
train loss: 14.175754  [12800/76380]
train loss: 3.916597  [16000/76380]
train loss: 10.744521  [19200/76380]
train loss: 8.070532  [22400/76380]
train loss: 7.056178  [25600/76380]
train loss: 8.108053  [28800/76380]
train loss: 17.218472  [32000/76380]
train loss: 17.432703  [35200/76380]
train loss: 4.995706  [38400/76380]
train loss: 5.760689  [41600/76380]
train loss: 7.863396  [44800/76380]
train loss: 8.857998  [48000/76380]
train loss: 6.062691  [51200/76380]
train loss: 9.302526  [54400/76380]
train loss: 8.270498  [57600/76380]
train loss: 4.884835  [60800/76380]
train loss: 5.627854  [64000/76380]
train loss: 6.241453  [67200/76380]
train loss: 11.312472  [70400/76380]
train loss: 5.972509  [73600/76380]
Avg validation loss: 10.225483
Epoch 5
-------------------------------
train loss: 5.555001  [    0/76380]
train loss: 10.065501  [ 3200/76380]
train loss: 11.965783  [ 6400/76380]
train loss: 3.554940  [ 9600/76380]
train loss: 7.880834  [12800/76380]
train loss: 3.435062  [16000/76380]
train loss: 4.884480  [19200/76380]
train loss: 7.186940  [22400/76380]
train loss: 5.943693  [25600/76380]
train loss: 6.537017  [28800/76380]
train loss: 6.953632  [32000/76380]
train loss: 4.855757  [35200/76380]
train loss: 8.877319  [38400/76380]
train loss: 3.424611  [41600/76380]
train loss: 5.197535  [44800/76380]
train loss: 10.557340  [48000/76380]
train loss: 2.688172  [51200/76380]
train loss: 10.599802  [54400/76380]
train loss: 8.836698  [57600/76380]
train loss: 8.859166  [60800/76380]
train loss: 5.520281  [64000/76380]
train loss: 7.311696  [67200/76380]
train loss: 5.290305  [70400/76380]
train loss: 9.768489  [73600/76380]
Avg validation loss: 5.753148
Epoch 6
-------------------------------
train loss: 5.469596  [    0/76380]
train loss: 5.398974  [ 3200/76380]
train loss: 6.203983  [ 6400/76380]
train loss: 6.739510  [ 9600/76380]
train loss: 5.522184  [12800/76380]
train loss: 7.102038  [16000/76380]
train loss: 7.308325  [19200/76380]
train loss: 6.247676  [22400/76380]
train loss: 7.859428  [25600/76380]
train loss: 5.675887  [28800/76380]
train loss: 4.089532  [32000/76380]
train loss: 14.112139  [35200/76380]
train loss: 10.580246  [38400/76380]
train loss: 3.376179  [41600/76380]
train loss: 8.671428  [44800/76380]
train loss: 11.973881  [48000/76380]
train loss: 8.893580  [51200/76380]
train loss: 5.397800  [54400/76380]
train loss: 7.356181  [57600/76380]
train loss: 6.985360  [60800/76380]
train loss: 6.026967  [64000/76380]
train loss: 4.667001  [67200/76380]
train loss: 7.824023  [70400/76380]
train loss: 5.277600  [73600/76380]
Avg validation loss: 7.156499
Epoch 7
-------------------------------
train loss: 4.832817  [    0/76380]
train loss: 7.855277  [ 3200/76380]
train loss: 8.937003  [ 6400/76380]
train loss: 6.014978  [ 9600/76380]
train loss: 3.653702  [12800/76380]
train loss: 3.876930  [16000/76380]
train loss: 5.225791  [19200/76380]
train loss: 12.361232  [22400/76380]
train loss: 2.460513  [25600/76380]
train loss: 4.067125  [28800/76380]
train loss: 10.390006  [32000/76380]
train loss: 6.272523  [35200/76380]
train loss: 5.466101  [38400/76380]
train loss: 5.513798  [41600/76380]
train loss: 7.350969  [44800/76380]
train loss: 4.667655  [48000/76380]
train loss: 7.689919  [51200/76380]
train loss: 8.001594  [54400/76380]
train loss: 5.026448  [57600/76380]
train loss: 4.665183  [60800/76380]
train loss: 9.145559  [64000/76380]
train loss: 6.791659  [67200/76380]
train loss: 4.828364  [70400/76380]
train loss: 19.239016  [73600/76380]
Avg validation loss: 8.427678
Epoch 8
-------------------------------
train loss: 7.275249  [    0/76380]
train loss: 14.048099  [ 3200/76380]
train loss: 9.345324  [ 6400/76380]
train loss: 3.931465  [ 9600/76380]
train loss: 6.273456  [12800/76380]
train loss: 5.335641  [16000/76380]
train loss: 3.796602  [19200/76380]
train loss: 5.145679  [22400/76380]
train loss: 4.977246  [25600/76380]
train loss: 3.113480  [28800/76380]
train loss: 5.624368  [32000/76380]
train loss: 3.582291  [35200/76380]
train loss: 5.751542  [38400/76380]
train loss: 6.392295  [41600/76380]
train loss: 5.401406  [44800/76380]
train loss: 6.118479  [48000/76380]
train loss: 3.037620  [51200/76380]
train loss: 4.946992  [54400/76380]
train loss: 4.057932  [57600/76380]
train loss: 5.658516  [60800/76380]
train loss: 8.864745  [64000/76380]
train loss: 5.363992  [67200/76380]
train loss: 5.634142  [70400/76380]
train loss: 6.543637  [73600/76380]
Avg validation loss: 4.997336
Epoch 9
-------------------------------
train loss: 4.396390  [    0/76380]
train loss: 11.532333  [ 3200/76380]
train loss: 4.643928  [ 6400/76380]
train loss: 4.771251  [ 9600/76380]
train loss: 5.495303  [12800/76380]
train loss: 5.908015  [16000/76380]
train loss: 5.107002  [19200/76380]
train loss: 3.962493  [22400/76380]
train loss: 10.707199  [25600/76380]
train loss: 5.998653  [28800/76380]
train loss: 2.985466  [32000/76380]
train loss: 11.834295  [35200/76380]
train loss: 3.492102  [38400/76380]
train loss: 8.338201  [41600/76380]
train loss: 8.707045  [44800/76380]
train loss: 10.996910  [48000/76380]
train loss: 4.074491  [51200/76380]
train loss: 4.876664  [54400/76380]
train loss: 6.472918  [57600/76380]
train loss: 3.930309  [60800/76380]
train loss: 4.905890  [64000/76380]
train loss: 8.296825  [67200/76380]
train loss: 7.348921  [70400/76380]
train loss: 16.372570  [73600/76380]
Avg validation loss: 5.058367
Epoch 10
-------------------------------
train loss: 5.006849  [    0/76380]
train loss: 6.467669  [ 3200/76380]
train loss: 6.327474  [ 6400/76380]
train loss: 4.315619  [ 9600/76380]
train loss: 3.148418  [12800/76380]
train loss: 3.126499  [16000/76380]
train loss: 6.717283  [19200/76380]
train loss: 4.105481  [22400/76380]
train loss: 6.544027  [25600/76380]
train loss: 5.776355  [28800/76380]
train loss: 2.892189  [32000/76380]
train loss: 8.840246  [35200/76380]
train loss: 5.768533  [38400/76380]
train loss: 7.328162  [41600/76380]
train loss: 5.849114  [44800/76380]
train loss: 6.793077  [48000/76380]
train loss: 3.791339  [51200/76380]
train loss: 9.258593  [54400/76380]
train loss: 4.613593  [57600/76380]
train loss: 3.223122  [60800/76380]
train loss: 2.620860  [64000/76380]
train loss: 2.737535  [67200/76380]
train loss: 5.155551  [70400/76380]
train loss: 6.617199  [73600/76380]
Avg validation loss: 5.640460
Done!